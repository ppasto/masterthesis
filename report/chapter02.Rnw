% LaTeX file for Chapter 02
<<'preamble02',include=FALSE>>=
library(knitr) 
opts_chunk$set( 
    fig.path='figure/ch02_fig',    
    self.contained=FALSE,
    cache=!FALSE
) 
@


<<echo=FALSE>>=
library(knitr) 
opts_chunk$set( 
    fig.path='figure/ch02_fig',    
    self.contained=FALSE,
    cache=TRUE
) 
@



<<echo=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/ch02_fig',
               echo=TRUE, message=FALSE,
               fig.width=7, fig.height=7,  
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
) 
knitr::opts_chunk$set(highlight = FALSE)
options(width=74)
@ 

\chapter{Methodology} 

\section{Definitions}

The general notion of \textbf{Recruitment} in this Master Thesis refers to the number of patients (Counts) at the Eligibility, or Enrollment, or Randomization, or Statistical Analysis stage in Figure \ref{fig:2_1_a}. Figure \ref{fig:2_1_a} is a schematic representation of a CONSORT study flow chart \citep{schulz2010consort} inspired by a real CONSORT study flow chart in Figure \ref{fig:2_0}. We define \textbf{Accrual} as cumulative recruitment.

The \textbf{Target Population} is a specific group within the broader population, defined by attributes relevant to the research question. This group is focused on criteria that match the study's goals \citep{willie2024population}. Defining the target population allows researchers to refine their objectives and recruitment methods to align with the study's aims.


The \textbf{Eligibility} criteria are the specific requirements that individuals must meet to participate in a study. Eligible patients will be selected from the target population. Inclusion criteria specify the conditions that allow individuals to participate in the trial, particularly focusing on the medical condition of interest. Any other factors that limit eligibility are classified as exclusion criteria \citep{van2007eligibility}, conditions or circumstances that disqualify potential participants \citep{food2018evaluating}.


In clinical trials, \textbf{Enrollment} refers to the formal process of registering participants into a study after they have met all eligibility criteria and provided informed consent. This process includes verifying that each participant satisfies the inclusion and exclusion criteria outlined in the study protocol \citep{NIH2021}. It is important to distinguish between recruitment and enrollment. Recruitment involves identifying and inviting potential participants to join the study, whereas enrollment occurs after these individuals have been screened, consented, and officially registered into the trial \citep{frank2004current}. 

Once enrolled, participants are assigned to specific treatment groups or interventions as defined by the study design. The most common practice is \textbf{Randomization}. In clinical research, randomization is the process of assigning participants to different treatment groups using chance methods, such as random number generators or coin flips \citep{lim2019randomization}. Randomized controlled trials (RCTs) are considered the most effective method for preventing bias in the evaluation of new interventions, drugs, or devices. \citep{van2007eligibility}.


In clinical research, \textbf{Statistical Analysis} involves applying statistical methods to collect, summarize, interpret, and present data derived from clinical studies. This process is essential for evaluating the safety, efficacy, and overall outcomes of medical interventions, ensuring that conclusions drawn are both reliable and valid \citep{panos2023statistical}. Not all participants who are randomized may be included in the final statistical analysis due to protocol deviations of patients not adhering to the protocol \citep{rehman2020exclusion}, missing data \citep{shih2002problems} or loss-to-follow-up, some participants may become unreachable or withdraw consent during the study, resulting in missing outcome data \citep{nuesch2009effects}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_a.png}
  \caption{Patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_a}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{whelan_flow.png}
  \caption{CONSORT study flow-chart \citep{whelan2018high, schulz2010consort}.}
  \label{fig:2_0}
\end{figure}

The number of patients decreases at each stage of a clinical study, from defining the target population to final statistical analysis, see Figure \ref{fig:2_1_b}. This process is known as patient leakage \citep{desai2014preventing}, alternative terms are attrition or retention.

Figure \ref{fig:2_1_a} generalizes the notion of patient leakage found in trial profiles such as the one found in Figure \ref{fig:2_0}. Figure \ref{fig:2_1_a} outlines the various stages of a clinical trial and analyzes the key factors contributing to patient attrition as they transition from one stage to the next.

Eligibility criteria narrow down participants, and enrollment further reduces numbers as only those meeting strict criteria are registered. Randomization assigns individuals to treatment groups, but some may later be excluded due to protocol deviations, missing data, or loss to follow-up. 


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_b.png}
  \caption{Visual representation of patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_b}
\end{figure}

\section{Uncertainty and models for counts and time}

There are two types of uncertainty, aleatory and epistemic \citep{ohagan2006}. The \textbf{Aleatory Uncertainty} reflects randomness that is inherent, irreducible and unpredictable in nature. \textbf{Epistemic Uncertainty} arises primarily from limited or imperfect knowledge about the parameters of a statistical model and can reflect fluctuations of the parameter. Obtaining more or better information about the parameter typically reduces the epistemic uncertainty. 


Let us denote

\begin{itemize}
\item $T=time$
\item $C=counts$
\item $\lambda=\frac{C}{T}$
\end{itemize}

We define \textbf{Recruitment Rate} $\lambda=\frac{C}{T}$ at which patients are collected, measured as persons per unit of time, where \textbf{Rate} is understood as a ratio in which the numerator and denominator are incremental differences \citep{piantadosi2024clinical}:

\begin{align*}
\lambda = \frac{\Delta C}{\Delta T} = \frac{C_1 - C_0}{T_1 - T_0} = \frac{C_1 - 0}{T_1 - 0} = \frac{C_1}{T_1}
\end{align*}
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $C = \lambda  $ & $\lambda  $ & 0 & No & No \\
% Poisson & $C \sim \textrm{Po} (\lambda )$ & $\lambda $ & $\lambda  $ & Yes & No \\
% Poisson-Gamma & $C \sim \textrm{Po} (\Lambda )$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha(\beta+1)}{\beta^2}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty of recruitment in one unit of time recruitment covered by different models for counts.}
% \label{tab:count_modeling0}
% \end{table}
% 
% 
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $C(t) = \lambda  t$ & $\lambda  t$ & 0 & No & No \\
% Poisson & $C(t) \sim \textrm{Po} (\lambda  t)$ & $\lambda  t$ & $\lambda  t$ & Yes & No \\
% Poisson-Gamma & $C(t) \sim \textrm{Po} (\Lambda  t)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $t\frac{\alpha}{\beta}$ & $t\frac{\alpha(\beta+t)}{\beta^2}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty in accrual until $t$ covered by different models for counts.}
% \label{tab:count_modeling_20}
% \end{table}
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Time} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $T(c) = c/\lambda$ & $c/\lambda$ & 0 & No & No \\
% Erlang & $T(c) \sim \textrm{G}(c, \lambda) $ & $c/\lambda$ & $c/\lambda^2$ & Yes & No \\
% Gamma-Gamma & $T(c) \sim \textrm{G}(c, \Lambda)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $c\frac{\beta}{\alpha-1}$ & $\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty of recruitment covered by different models for time having a fixed sample size $c$.}
% \label{tab:time_modeling0}
% \end{table}


Methods in Tables \ref{tab:count_modeling}, \ref{tab:count_modeling_2} and \ref{tab:time_modeling} are applicable to each level of recruitment in Figures \ref{fig:2_1_a} and \ref{fig:2_1_b}.

\chapter{Methods for Counts} 
\section{Counts: Model based on Expectations}
\label{sec:expect}

If we fix the duration of a study at time $T$ and we expect that we collect $C$ patients until $T$, we deterministically predict the recruitment rate per one unit of time (without taking into consideration any uncertainty) to be $\hat{\lambda}=\frac{C}{T}$ \citep{carter2004application}. 




\subsection{Expected recruitment in one unit of time}

Assume that the recruitment in one unit of time is constant and equal to $\lambda$. Then, $C = \textrm{E}C = \textrm{E}\lambda = \lambda$ and $\textrm{Var}(C) = \textrm{Var}(\lambda) = 0$, as we can see in Table \ref{tab:count_modeling}.

\subsection{Expected accrual at time point $t$}

Assuming that recruitments in each unit of time are independent of each other we have:

$C(t) = \textrm{E}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = \textrm{E}(\lambda t) = \lambda t$\\
$\textrm{Var}(C(t)) = \textrm{Var}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = t \textrm{Var}(\lambda) = 0$

Both the expected accrual and its zero-variance are recorded in Table \ref{tab:count_modeling_2}
and visualized in Figure \ref{fig:2_2} and Figure \ref{fig:2_5}.

\subsection{Criticism}

This is a simple deterministic method based on a linear extrapolation of the constant expected recruitment also called "First Order Recruitment Model" (FORM) \citep{comfort2013}. It is also referred to as the \textit{unconditional approach} and its main limitation is the lack of a mechanism to account for known sources of variation in the rate \citep{carter2005practical}. 

The model based on expectations is overly simplistic and fails to account for changes in center recruitment or the regulatory environment \citep{barnard2010systematic}. Therefore, stochastic models that incorporate randomness in the recruitment process are more suitable than the widely used deterministic approach \citep{zhang2012modeling}.



\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C = \lambda  $ & $\lambda  $ & 0 & No & No \\
Poisson & $C \sim \textrm{Po} (\lambda )$ & $\lambda $ & $\lambda  $ & Yes & No \\
Poisson-Gamma & $C \sim \textrm{Po} (\Lambda )$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha(\beta+1)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty of recruitment in one unit of time recruitment covered by different models for counts.}
\label{tab:count_modeling}
\end{table}




\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C(t) = \lambda  t$ & $\lambda  t$ & 0 & No & No \\
Poisson & $C(t) \sim \textrm{Po} (\lambda  t)$ & $\lambda  t$ & $\lambda  t$ & Yes & No \\
Poisson-Gamma & $C(t) \sim \textrm{Po} (\Lambda  t)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $t\frac{\alpha}{\beta}$ & $t\frac{\alpha(\beta+t)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty in accrual until $t$ covered by different models for counts.}
\label{tab:count_modeling_2}
\end{table}


\section{Counts: Model based on Poisson Process}

One way to incorporate variation in the mean number of participants per day is by assuming that participants are recruited according to a known probability distribution, such as the Poisson distribution \citep{carter2004application}. This approach emulates trial accrual using a random number generator and records the time required to reach the target sample size over multiple iterations \citep{carter2005practical}. 

The resulting distribution helps estimate the probability of completing accrual within a given time frame and assess variability in accrual time. This method is particularly useful when a trial has a fixed duration, as it allows researchers to determine the necessary number of clinical centers and monthly recruitment rate to achieve a high probability (e.g., 80\%) of completing enrollment on time \citep{carter2005practical}.

The Poisson distribution $C\sim \rm{Po} (\lambda)$ allows us to explain the recruitment of patients. It is a discrete variable that expresses the probability of a given number of events (in our case, patient recruitment) occurring in a fixed unit interval of time. We assume that these events occur with a known constant rate $\lambda$ and are independent of each other.

\begin{align*}
\textrm{P}[C=&c] = \frac{\lambda^c}{c!}e^{-\lambda} \\
&c = 0,1,2,\ldots
\end{align*}


One important property from the Poisson distribution is that it is infinitely divisible \citep{held2014applied}. If $X_i\sim \textrm{Po} (\lambda_i)$ for $i=1,\ldots, n$ are independent, then, $\sum_{i=1}^n X_i \sim \textrm{Po} \Big( \sum_{i=1}^n \lambda_i \Big)$.

\subsection{Recruitment in one unit of time}

The recruitment of patients in one unit of time follows $C\sim \textrm{Po} (\lambda)$ and the expectation and variance are
$\textrm{E}C = \lambda$ and $\textrm{Var}(C) = \lambda$, as we can see in Table \ref{tab:count_modeling}

\subsection{Accrual at time point $t$}
At time point $t$, the accrual follows $C\sim \textrm{Po} (\lambda t)$. Using the infinitely divisible property from the Poisson applicable to independent random variables, $\underbrace{\textrm{Po} (\lambda) +\cdots +\textrm{Po} (\lambda)}_{t \ \text{times}} = \textrm{Po} (\lambda t)$. We assume that the recruitment of patients in $t$ unit time intervals is independent from another. As we can see in Table \ref{tab:count_modeling_2}, the expectation and variance are the following:

\begin{align*}
\textrm{E}C(t) & = \lambda t \\
\textrm{Var}(C(t)) & = \lambda t
\end{align*}

For example, if we assume $\lambda = 0.591$ per day and $t=550$, we can show the accrual of 100 different studies in Figure \ref{fig:2_2} and the histogram at $t=550$ days. The exact distribution at $t=550$ is provided in Figure \ref{fig:2_3} and the Cummulative Distribution Function (CDF) in Figure \ref{fig:2_4}. The uncertainty bands based on the exact quantiles are displayed in Figure \ref{fig:2_5}.

\begin{figure}
<<echo=FALSE>>=
# 1000 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))

for (i in 1:n) {
	cval <- rpois(length(t), lambda)  
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins

plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count")

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("red", "black"),
			 lty = c(2, 1),
			 lwd = c(2,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(28, 0.01, 2.1, 2.1))  # Minimize margins
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Poisson-distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from section \ref{sec:expect}, while the red dashed lines indicate Poisson's 95\% aleatory uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_2}
\end{figure}


\begin{figure}
<<echo=FALSE>>=
bar_positions <- barplot(dpois(200:500, lambda*550), 
												 main = "Probability Mass Function (PMF)", 
												 xlab = "Counts", 
												 xaxt = "n")  

axis(1, at = seq(1, length(200:500), by = 50), labels = seq(200, 500, by = 50))  
@   
  \caption{Probability Mass Function (PMF) of Poisson-distributed counts: This bar plot represents the probability mass function (PMF) of counts ranging from 200 to 500, using a Poisson distribution $\textrm{Po}(\lambda t)$ with a rate parameter $\lambda = 0.591$ per day at time $t = 550$ days.}
  \label{fig:2_3}
\end{figure}

\begin{figure}
<<echo=FALSE>>=
probdist <- dpois(200:500, lambda*550)
cdf <- cumsum(probdist)
cdf_positions <- barplot(cdf, 
												 main = "Cummulative distribution", 
												 xlab = "Counts", 
												 xaxt = "n")
axis(1, at = seq(1, length(200:500), by = 50), labels = seq(200, 500, by = 50))  
@   
  \caption{Cumulative Distribution Function (CDF) of Poisson-distributed counts: The bar plot illustrates the cumulative probability distribution for counts within the range of 200 to 500, using a Poisson $\textrm{Po}(\lambda t)$ distribution with a rate parameter $\lambda = 0.591$ per day at time $t=550$ days.}
  \label{fig:2_4}
\end{figure}


\begin{figure}
<<echo=FALSE>>=

# Compute Poisson quantiles over time
q_975_line <- qpois(0.975, lambda * t)
q_900_line <- qpois(0.900, lambda * t)
q_750_line <- qpois(0.75, lambda * t)
q_250_line <- qpois(0.25, lambda * t)
q_100_line <- qpois(0.1, lambda * t)
q_025_line <- qpois(0.025, lambda * t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
     main = "Quantiles")

# Fill the area with a red gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(255/255, 182/255, 193/255, 0.4), border = NA) # Light Red (pink)
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(220/255, 20/255, 60/255, 0.4), border = NA)  # Crimson (medium red)
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(139/255, 0/255, 0/255, 0.4), border = NA)    # Dark Red (maroon)
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(220/255, 20/255, 60/255, 0.4), border = NA)  # Crimson (medium red)
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(255/255, 182/255, 193/255, 0.4), border = NA) # Light Red (pink)

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  

# Legend
legend("topleft",
       legend = c("2.5th - 97.5th Percentile [95%]", 
                  "10th - 90th Percentile     [80%]", 
                  "25th - 75th Percentile     [50%]", 
                  "Expected Accrual"),
       col = c(rgb(255/255, 182/255, 193/255, 0.4), rgb(220/255, 20/255, 60/255, 0.4), rgb(139/255, 0/255, 0/255, 0.4), "black"),
       pch = c(15, 15, 15, NA), lty = c(NA, NA, NA, 1), lwd = c(NA, NA, NA, 2),
       bg = "white",
       cex = 0.7)

@   
  \caption{Predicted uncertainty bands for Poisson process with $\lambda = 0.591$ per day. The black line represents the expected accrual, while the red shaded regions indicate aleatory uncertainty: the dark red band spans the interquantile range (25th - 75th percentiles), the lighter red band cover the 10th - 90th percentile range and the light red the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_5}
\end{figure}

\subsection{Criticism}

This model was used by \cite{carter2004application} and \cite{carter2005practical}. Although this model accounts for aleatory uncertainty, the recruitment rate is assumed to be constant for the entire period of time. Therefore, an alternative method that accounts for varying recruitment rates over time is necessary.

\section{Counts: Negative Binomial model derived from Poisson-Gamma model}

The basic Poisson model does not account for variations in recruitment rates or uncertainties in rate estimates \citep{mountain2022recruitment}. To address this \cite{anisimov2007modelling} propose a random effects model where recruitment follows a homogeneous Poisson process with rates drawn from a gamma distribution, a probabilistic model with Poisson-Gamma mixture distribution. In this approach, rates are treated as random variables with a prior gamma distribution, whose parameters are estimated using current recruitment data. This allows for a posterior distribution of rates to be used for recruitment prediction in an empirical Bayesian framework.  

Building on this, a Poisson-Gamma model can be implemented by randomly generating recruitment rates using a Gamma distribution and plugging them into a Poisson process. This model can be used to make two types of projections: a point prediction estimating when the expected number of events will reach a specific sample size and a Bayesian interval prediction based on the generation of future accrual and event dates. This method enables flexible forecasting for any milestone at any calendar time \citep{bagiella2001predicting}.


\begin{figure}
<<echo = FALSE,  fig.width=7, fig.height=5>>=
par(mfrow=c(1,3))
curve(dgamma(x, shape = 324, rate = 548), 
			from = 0, to = 1,
			main = "Prior", 
			xlab = "Recruitment rate", 
			ylab = "",
			col = "red")

curve(dgamma(x, shape = 32.4, rate = 54.8), add = TRUE,
			col = "blue")

curve(dgamma(x, shape = 3.24, rate = 5.48), add = TRUE,
			col = "green")

legend("topleft",
			 legend = c(expression("G (" ~ alpha == 324 ~ beta == 548 ~ ")"),
			 						expression("G (" ~ alpha == 32.4 ~ beta == 54.8 ~ ")"),
			 						expression("G (" ~ alpha == 3.24 ~ beta == 5.48 ~ ")")),
			 col = c("red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)


plot(0:5, dpois(0:5, lambda), 
		 type = "l",
		 main = "PMF", 
		 xlab = "Counts", 
		 ylab = "")


# lines(0:5, dnbinom(0:5, size = 324, mu = lambda), col = "red")
# lines(0:5, dnbinom(0:5, size = 32.4, mu = lambda), col = "blue")
# lines(0:5, dnbinom(0:5, size = 3.24, mu = lambda), col = "green")

lines(0:5, dnbinom(0:5, size = 324, prob = 548/(548+1)), col = "red")
lines(0:5, dnbinom(0:5, size = 32.4, prob = 54.8/(54.8+1)), col = "blue")
lines(0:5, dnbinom(0:5, size = 3.24, prob = 5.48/(5.48+1)), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(PoG ~ (alpha == 324 ~ beta == 548)),
                  expression(PoG ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(PoG ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)


plot(0:5, ppois(0:5, lambda), 
		 type = "l",
		 main = "CDF", 
		 xlab = "Counts", 
		 ylab = "")



# lines(0:5, pnbinom(0:5, size = 324, mu = lambda), col = "red")
# lines(0:5, pnbinom(0:5, size = 32.4, mu = lambda), col = "blue")
# lines(0:5, pnbinom(0:5, size = 3.24, mu = lambda), col = "green")

lines(0:5, pnbinom(0:5, size = 324, prob = 548/(548+1)), col = "red")
lines(0:5, pnbinom(0:5, size = 32.4, prob = 54.8/(54.8+1)), col = "blue")
lines(0:5, pnbinom(0:5, size = 3.24, prob = 5.48/(5.48+1)), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(PoG ~ (alpha == 324 ~ beta == 548)),
                  expression(PoG ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(PoG ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)

@
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Negative Binomial changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=1$.}
  \label{fig:2_6}
\end{figure}

\subsection{Recruitment in one unit of time}

Let $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. Where $\Lambda$ represents the "recruitment proneness" in the unit of time. 

\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{\lambda^c\exp(-\lambda)}{c!}\Bigg[\lambda^{\alpha-1}\exp(-\beta\lambda)\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c)}{c!\Gamma(\alpha) (\beta+1)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+1)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+1)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots
\end{align*}

Thus, $C\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+1}\Bigg)$.


The parameter $\lambda$ in the integral represents the expected recruitment rate per unit of time for an individual and this recruitment rate is assumed to vary from individual to individual as it is generated by a $\textrm{G}(\alpha,\beta)$ distribution \citep{johnson2005univariate}.


Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}C &= \textrm{E}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda] = \alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_C(C|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda] + \textrm{E}_{\Lambda}[\Lambda] \\
&=\alpha/\beta^2 + \alpha/\beta = \frac{\alpha(\beta+1)}{\beta^2}
\end{align*}


There are two different interpretations of the Negative Binomial, Failure-Based and Count-based.
\subsection{Failure-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}(r,\pi)$ models the number of \textbf{failures} before achieving a fixed number of \textbf{successes} in a sequence of Bernoulli trials. 
\item Parametrization:
	\begin{itemize}
	\item $n$: Number of successes to be achieved (fixed).
	\item $p$: Probability of success in each trial
	\item The random variable $X$ represents the number of failures before achieving $n$ successes.
	\end{itemize}
\item Probability Mass Function (PMF) \citep{R-stats}:
\begin{align*}
\textrm{P}(X=&x) = \frac{\Gamma(x+n)}{\Gamma(x) x!}  p^n(1-p)^x, \\
&x = 0,1,2, \ldots, n >0\\
&0<p\leq 1
\end{align*}
where $x$ is the number of failures.
\item Interpretation: In a sequence of independent binary trials with constant probability $p$ of observing a \textit{non-recruited} patient, $X$ is the number of \textit{recruited} patients observed at the time that $x$ \textit{non-recruited} patients are observed \citep{meeker2017statistical}.


\end{enumerate}

With respect to the parameters, $n>0$ represents the number of successes until 
the experiment is stopped. The success probability in each experiment is 
represented by $p\in[0,1]$.  In R the functions \_\texttt{nbinom(..., size = $n$, prob = $p$)} relate to the random variable $X-r$, the number of successes (as opposed to the number of trials) until $r$ successes have been achieved \citep{held2014applied}. 

\begin{align*}
EX & = \frac{r(1-\pi)}{\pi}\\
Var(X) & = \frac{r(1-\pi)}{\pi^2}
\end{align*}

Since we will be using the Count-Based interpretation of the Negative Binomial \citep{hilbe2011negative}, our parametrization relates to R with $n = \alpha$ and $p = \frac{\beta}{\beta+1}$.

\subsection{Count-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}\Bigg(\alpha,\frac{\beta}{\beta+1}\Bigg)$ can also be seen as a Poisson-Gamma mixture, where the observed count data follows a Poisson distribution with a mean that itself follows a Gamma distribution, $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. 
\item Parametrization:
	\begin{itemize}
	\item $\mu = \frac{\alpha}{\beta}$: Mean of the distribution (expected number of occurrences).
	\item $\alpha$: Dispersion parameter, controlling the variance.
	\end{itemize}
\item Alternative formulation of the PMF \citep{hilbe2011negative}:
\begin{align*}
\textrm{P}(X=&c) = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\beta+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha}, \\
&c\geq 0 
\end{align*}
where $c$ is the counts.
\item Interpretation: This model is used to represent "recruitment proneness". The parameter $\mu$ represents the expected number of recruitments in a study \citep{johnson2005univariate}.

\end{enumerate}

In the count-based method we take into account the overdispersion of the data. When the variability in the observed data is greater than what is expected.

\begin{align*}
\textrm{E}X&=\mu\\
\textrm{Var}X&=\mu + \frac{\mu}{\beta}
\end{align*}

How do we get to our formulation of the PMF:

\begin{align*}
\textrm{P}(X=c) & = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\alpha+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha}{\alpha\beta+\alpha}\Bigg)^{c} \Bigg(\frac{\beta\alpha}{\alpha\beta+\alpha}\Bigg)^{\alpha} \\
&= \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}
\end{align*}



% 
% \begin{align*}
% Mean &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha(\beta+1)}{\beta(\beta+1)}\\
% &= \frac{\alpha}{\beta}
% \end{align*}
% 
% \begin{align*}
% Variance &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha(\beta+1)^2}{\beta^2(\beta+1)}\\
% &= \frac{\alpha(\beta+1)}{\beta^2}
% \end{align*}


\begin{figure}
<<echo=FALSE>>=
# 100 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

alpha <- 324
beta <- 548
# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
v_lambda <- rgamma(n, shape = alpha, rate = beta)

for (i in 1:n) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins
plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count")

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qnbinom(p = 0.975, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")
lines(t, qnbinom(p = 0.025, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")

legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("green", "black"),
			 lty = c(2, 1),
			 lwd = c(2,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(27, 0.1, 2.1, 1))  # Adjust margins for visibility
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Poisson-Gamma $(\alpha = 324, \beta = 548)$ distributed counts with $\mu = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the red dashed lines indicate Poisson-Gamma 95\% aleatory and epistemic uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_7}
\end{figure}



\begin{figure}
<<echo=FALSE>>=
n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 324
beta <- 548

# Compute Poisson quantiles over time
q_975_line <- qnbinom(p = 0.975, size = alpha, mu = lambda*t)
q_900_line <- qnbinom(p = 0.900, size = alpha, mu = lambda*t)
q_750_line <- qnbinom(p = 0.75, size = alpha, mu = lambda*t)
q_250_line <- qnbinom(p = 0.25, size = alpha, mu = lambda*t)
q_100_line <- qnbinom(p = 0.1, size = alpha, mu = lambda*t)
q_025_line <- qnbinom(p = 0.025, size = alpha, mu = lambda*t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
		 main = "Quantiles")

# Fill the area with a green gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(0/255, 100/255, 0/255, 0.4), border = NA)    # Dark Green
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  

# Legend
legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]", 
			 					 "10th - 90th Percentile     [80%]", 
			 					 "25th - 75th Percentile     [50%]", 
			 					 "Expected Accrual"),
			 col = c(rgb(144/255, 238/255, 144/255, 0.4), rgb(50/255, 205/255, 50/255, 0.4), rgb(0/255, 100/255, 0/255, 0.4), "black"),
			 pch = c(15, 15, 15, NA), lty = c(NA, NA, NA, 1), lwd = c(NA, NA, NA, 2),
			 bg = "white",
			 cex = 0.7)
@   
  \caption{Predicted uncertainty bands for Poisson-Gamma process with $\mu = 0.591$ per day. The black line represents the expected accrual, while the green shaded regions indicate aleatory and epistemic uncertainty: the dark green band spans the interquantile range (25th - 75th percentiles), the lighter green band cover the 10th - 90th percentile range and the light green the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_8}
\end{figure}

\subsection{Sensitivity Analysis}

In Figures \ref{fig:2_6b} and \ref{fig:2_6}, the Poisson distribution captures aleatory uncertainty, while the Gamma prior represents epistemic uncertainty. The Poisson-Gamma distribution incorporates both types of uncertainty. The sensitivity analysis, also shown in Figures \ref{fig:2_6b} and \ref{fig:2_6}, highlights that while the expectation remains unchanged, smaller parameter values lead to greater overall uncertainty due to the increased variance introduced by the Gamma distribution. The smaller the $\beta$, the greater the variance.


\begin{align*}
\textrm{Var}(\textrm{G}(\alpha, \beta)) = \frac{\alpha}{\beta^2} =\frac{\textrm{E}(\textrm{G}(\alpha, \beta))}{\beta}
\end{align*}

In the Poisson-Gamma model we can interpret $\alpha$ as the parameter that represents the sample size and $\beta$ represents time.

\begin{figure}
<<echo = FALSE,  fig.width=7, fig.height=5>>=
par(mfrow=c(1,3))
curve(dgamma(x, shape = 324, rate = 548), 
			from = 0, to = 1,
			main = "Prior", 
			xlab = "Recruitment rate", 
			ylab = "",
			col = "red")

curve(dgamma(x, shape = 32.4, rate = 54.8), add = TRUE,
			col = "blue")

curve(dgamma(x, shape = 3.24, rate = 5.48), add = TRUE,
			col = "green")

legend("topleft",
			 legend = c(expression("G (" ~ alpha == 324 ~ beta == 548 ~ ")"),
			 						expression("G (" ~ alpha == 32.4 ~ beta == 54.8 ~ ")"),
			 						expression("G (" ~ alpha == 3.24 ~ beta == 5.48 ~ ")")),
			 col = c("red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)


plot(200:500, dpois(200:500, lambda*550), 
		 type = "l",
		 main = "PMF", 
		 xlab = "Counts", 
		 ylab = "")


lines(200:500, dnbinom(200:500, size = 324, mu = lambda*550), col = "red")
lines(200:500, dnbinom(200:500, size = 32.4, mu = lambda*550), col = "blue")
lines(200:500, dnbinom(200:500, size = 3.24, mu = lambda*550), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(PoG ~ (alpha == 324 ~ beta == 548)),
                  expression(PoG ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(PoG ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)


plot(200:500, ppois(200:500, lambda*550), 
		 type = "l",
		 main = "CDF", 
		 xlab = "Counts", 
		 ylab = "")



lines(200:500, pnbinom(200:500, size = 324, mu = lambda*550), col = "red")
lines(200:500, pnbinom(200:500, size = 32.4, mu = lambda*550), col = "blue")
lines(200:500, pnbinom(200:500, size = 3.24, mu = lambda*550), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(PoG ~ (alpha == 324 ~ beta == 548)),
                  expression(PoG ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(PoG ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)

@
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Poisson-Gamma changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=550$.}
  \label{fig:2_6b}
\end{figure}

\subsection{Accrual at time point $t$}

Let $C(t)|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. By the infinitely divisible property of the Poisson distribution we have that $\underbrace{\textrm{Po} (\Lambda) +\cdots +\textrm{Po} (\Lambda)}_{t \ \text{times}} = \textrm{Po} (\Lambda t)$. This Poisson-Gamma model has parameters $t$, $\alpha$ and $\beta$. Hence, we denote it as $C(t)\sim \textrm{PoG}(t, \alpha, \beta)$.

\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda t) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{(\lambda t)^c\exp(-\lambda t)}{c!}\Bigg[(\lambda)^{\alpha-1}\exp(-\beta\lambda )\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha t^c}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda t)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c) t^c}{c!\Gamma(\alpha) (\beta+t)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+t)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+t)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha t^c\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+t}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{t}{\beta+t}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+t}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots
\end{align*}
Thus, $C(t)\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+t}\Bigg)$

We will be focusing on the count-based interpretation of the Negative Binomial distribution. We can relate this interpretation to the more popular failure-based interpretation by seeing $\alpha$ as $n$, the number of successes. In our case, the number of patients we wish to recruit. And, $p = \frac{\beta}{\beta+t}$ which is the probability of success, as the probability of recruiting.


Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C(t)|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}(C(t)) &= \textrm{E}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda t] = t\alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C(t)) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_{C(t)}(C(t)|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda t] + \textrm{E}_{\Lambda}[\Lambda t] \\
&=t^2\alpha/\beta^2 + t\alpha/\beta = \frac{t\alpha(\beta+t)}{\beta^2}
\end{align*}

Therefore, we clearly have overdispersion $\textrm{Var}(C(t))>\textrm{E}(C(t))$. This can be easily seen because:

\begin{align*}
\textrm{Var}(C(t)) &=\textrm{E}(C(t))\frac{\beta+t}{\beta}\\
&=\textrm{E}(C(t))\Bigg ( 1+\frac{t}{\beta} \Bigg)
\end{align*}

\section{Comparison of models for the accrual of counts}

As we can see in Figures \ref{fig:2_10} and \ref{fig:2_11}, we are taking into account more epistemic uncertainty of the recruitment rate $\Lambda$ in the Poisson-Gamma model than we do in the Poisson process where $\lambda$ is fixed. Hence, the color-coding, red for only aleatory and green, aleatory and epistemic. In reality we expect fluctuations of recruitment rates, therefore, the Poisson-Gamma model is more realistic.

Figures \ref{fig:2_10} and \ref{fig:2_11} assume that the recruitment rates do not vary much by assuming $\textrm{G}(\alpha = 324, \beta = 548)$ as prior, where the expectation is $0.591$ and the standard deviation $0.033$. In contrast, Figures \ref{fig:2_10a} and \ref{fig:2_11a}, assume a less informative prior $\textrm{G}(\alpha = 32.4, \beta = 54.8)$ of the recruitment rates with expectation $0.591$ and empirical standard deviation $0.104$, showing a more pronounced discrepancy between the Poisson and the Poisson-Gamma process.

\begin{figure}
<<echo=FALSE>>=
# 100 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

alpha <- 324
beta <- 548
# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
v_lambda <- rgamma(n, shape = alpha, rate = beta)

for (i in 1:n) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins
plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count")

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qnbinom(p = 0.975, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")
lines(t, qnbinom(p = 0.025, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

legend("topleft",
			 legend = c("PoG 2.5th - 97.5th Percentile [95%]",
			 					 "Po 2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("green", "red", "black"),
			 lty = c(2, 2, 1),
			 lwd = c(2,2,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(27, 0.1, 2.1, 1))  # Adjust margins for visibility
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Comparison of the 95\% uncertainty range taken into consideration in the Poisson-Gamma $(\alpha = 324, \beta = 548)$ model as opposed to the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_10}
\end{figure}


\begin{figure}
<<echo=FALSE>>=
# 100 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

alpha <- 32.4
beta <- 54.8
# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
v_lambda <- rgamma(n, shape = alpha, rate = beta)

for (i in 1:n) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins
plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count",
		 ylim = c(0, 500))

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qnbinom(p = 0.975, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")
lines(t, qnbinom(p = 0.025, size = alpha, mu = lambda*t), lty = 2, lwd = 2, col = "green")
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

legend("topleft",
			 legend = c("PoG 2.5th - 97.5th Percentile [95%]",
			 					 "Po 2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("green", "red", "black"),
			 lty = c(2, 2, 1),
			 lwd = c(2,2,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(18, 0.1, 2.1, 1))  # Adjust margins for visibility
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Comparison of the 95\% uncertainty range taken into consideration in the Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ model as opposed to the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_10a}
\end{figure}

\begin{figure}
<<echo=FALSE>>=
n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 324
beta <- 548

# Compute Poisson quantiles over time
q_975_line <- qnbinom(p = 0.975, size = alpha, mu = lambda*t)
q_900_line <- qnbinom(p = 0.900, size = alpha, mu = lambda*t)
q_750_line <- qnbinom(p = 0.75, size = alpha, mu = lambda*t)
q_250_line <- qnbinom(p = 0.25, size = alpha, mu = lambda*t)
q_100_line <- qnbinom(p = 0.1, size = alpha, mu = lambda*t)
q_025_line <- qnbinom(p = 0.025, size = alpha, mu = lambda*t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
		 main = "Quantiles")

# Fill the area with a green gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(0/255, 100/255, 0/255, 0.4), border = NA)    # Dark Green
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

# Legend
legend("topleft",
			 legend = c("PoG 2.5th - 97.5th Percentile [95%]", 
			 					 "PoG 10th - 90th Percentile     [80%]", 
			 					 "PoG 25th - 75th Percentile     [50%]", 
			 					 "Expected Accrual",
			 					 "Po 2.5th - 97.5th Percentile [95%]"),
			 col = c(rgb(144/255, 238/255, 144/255, 0.4), rgb(50/255, 205/255, 50/255, 0.4), rgb(0/255, 100/255, 0/255, 0.4), "black", "red"),
			 pch = c(15, 15, 15, NA, NA), lty = c(NA, NA, NA, 1, 2), lwd = c(NA, NA, NA, 2, 2),
			 bg = "white",
			 cex = 0.7)
@   
  \caption{Comparison of the theoretical quantiles in the Poisson-Gamma $(\alpha = 324, \beta = 548)$ model and the 95\% uncertainty range of the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_11}
\end{figure}


\begin{figure}
<<echo=FALSE>>=
n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 32.4
beta <- 54.8

# Compute Poisson quantiles over time
q_975_line <- qnbinom(p = 0.975, size = alpha, mu = lambda*t)
q_900_line <- qnbinom(p = 0.900, size = alpha, mu = lambda*t)
q_750_line <- qnbinom(p = 0.75, size = alpha, mu = lambda*t)
q_250_line <- qnbinom(p = 0.25, size = alpha, mu = lambda*t)
q_100_line <- qnbinom(p = 0.1, size = alpha, mu = lambda*t)
q_025_line <- qnbinom(p = 0.025, size = alpha, mu = lambda*t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
		 main = "Quantiles")

# Fill the area with a green gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(0/255, 100/255, 0/255, 0.4), border = NA)    # Dark Green
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

# Legend
legend("topleft",
			 legend = c("PoG 2.5th - 97.5th Percentile [95%]", 
			 					 "PoG 10th - 90th Percentile     [80%]", 
			 					 "PoG 25th - 75th Percentile     [50%]", 
			 					 "Expected Accrual",
			 					 "Po 2.5th - 97.5th Percentile [95%]"),
			 col = c(rgb(144/255, 238/255, 144/255, 0.4), rgb(50/255, 205/255, 50/255, 0.4), rgb(0/255, 100/255, 0/255, 0.4), "black", "red"),
			 pch = c(15, 15, 15, NA, NA), lty = c(NA, NA, NA, 1, 2), lwd = c(NA, NA, NA, 2, 2),
			 bg = "white",
			 cex = 0.7)
@   
  \caption{Comparison of the theoretical quantiles in the Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ model and the 95\% uncertainty range of the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_11a}
\end{figure}

\section{Generation of Poisson-Gamma model}

There are two ways with which we can generate the random $\Lambda$ in the $\textrm{PoG}(t, \alpha, \beta)$ model and we will prove in this section how they are both equivalent.

\subsection{Version 1}

We generate a random vector of $\lambda$ values of length $M=10^5$ with $\textrm{G}(\alpha, \beta)$ and a vector of counts at each time point, of length $t$ with $\textrm{Po}(\lambda)$. For the final counts we sum cumulatively (cumsum) the counts at $t$ so that we get the accrual of patients at time $t$ of each study.

\begin{align*}
\lambda^m &\sim \textrm{G}(\alpha, \beta) \\
C(t)^m &\sim \textrm{Po} (\lambda^m t) = \underbrace{\textrm{Po} (\lambda^m) +\cdots +\textrm{Po} (\lambda^m)}_{t \ \text{times}}\\
& m = 1, \ldots, M
\end{align*}

\subsection{Version 2}

At each time point $i=1,\ldots, t$, we generate a $\lambda^{(i)}$ with $\textrm{G}(\alpha, \beta)$ and a count with $\textrm{Po}(\lambda^{(i)})$. For the final counts we sum cumulatively (cumsum) the counts at $t$ so that we get the accrual of patients at time $t$ of each study.

\begin{align*}
\lambda^{(i)m} &\sim \textrm{G}(\alpha, \beta) \\
C (i)^m&\sim \textrm{Po} (\lambda^{(i)m})\\
C(t)^m &= \sum_{i=1}^tC(i)^m \\
& i = 1, \ldots, t \\
& m = 1, \ldots, M 
\end{align*}


As we can see in Figure \ref{fig:2_12}, the histogram of these two versions overlap and therefore we can empirically conclude that both these ways of simulating the Poisson-Gamma model are equivalent.


\begin{figure}
<<echo=FALSE, cache = TRUE>>=
##################################### MORE EFFICIENT:
set.seed(2025)

n <- 10^5
t <- seq(1, 550, 1)
lambda <- 0.591

alpha <- 324
beta <- 548

### FIRST
v_lambda <- rgamma(n, shape = alpha, rate = beta)

# Use lapply and simplify2array instead of a for-loop
cval_cum_matrix <- t(simplify2array(lapply(v_lambda, function(l) cumsum(rpois(length(t), lambda = l)))))

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

### SECOND
# Vectorized approach for the second process
cval_cum_matrix_2 <- t(simplify2array(
	lapply(1:n, function(x) cumsum(rpois(length(t), lambda = rgamma(1, shape = alpha, rate = beta))))
))

# Extract final counts for histogram
final_counts_2 <- cval_cum_matrix_2[, length(t)]


# library(MASS)
# truehist(final_counts, col = rgb(1, 0, 0, 0.4), main = "Comparison", xlab = "Counts", ylab = "Density")
# par(new = TRUE)
# truehist(final_counts_2, col = rgb(0, 0, 1, 0.4), axes = FALSE, xlab = "", ylab = "")
# 
# legend("topright", legend = c("Version 1", "Version 2"), fill = c(rgb(1, 0, 0, 0.4), rgb(0, 0, 1, 0.4)))


plot(density(final_counts),
		 main = "Comparison",
		 xlab = "Counts",
		 col = rgb(1, 0, 0, 0.4), 
		 lwd = 4)

lines(density(final_counts_2), 
			lwd = 2,
			lty = 2,
			col = rgb(0, 0, 1, 0.4))
legend("topright", 
			 legend = c("Version 1", "Version 2"), 
			 col = c(rgb(1, 0, 0, 0.4), rgb(0, 0, 1, 0.4)),
			 lwd = c(4, 2),
			 lty = c(1, 2), 
			 cex = 0.7)
@   
  \caption{Comparison of the two versions with which we can generate a Poisson-Gamma model. Version 1: generates a vector of $\lambda$ with length $M=10^5$ using a $\textrm{G}(\alpha = 324, \beta = 548)$. Version 2: at each time point only one $\lambda$ is generated. We compare both these versions at accrual of time $t=550$ for $\lambda=0.591$.}
  \label{fig:2_12}
\end{figure}

\chapter{Methods for Waiting Time} 
\section{Time: Model based on Expectations}

If we fix the sample size of a study at $c$ and we expect the recruitment rate to be $\lambda$, we deterministically predict the time planned for the study (without taking into consideration any uncertainty) to be $\hat{T}=\frac{c}{\lambda}$ \citep{bagiella2001predicting}. Researchers tend to use simple unconditional methods \citep{white2015projection}. Regarding the expectation and variance in this framework: $\textrm{E}T = \textrm{E}(c/\lambda) = c/\lambda$ and $\textrm{Var}(T) = \textrm{Var}(c/\lambda) = 0$, as we can see in Table \ref{tab:time_modeling}.


\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Time} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $T(c) = c/\lambda$ & $c/\lambda$ & 0 & No & No \\
Erlang & $T(c) \sim \textrm{G}(c, \lambda) $ & $c/\lambda$ & $c/\lambda^2$ & Yes & No \\
Gamma-Gamma & $T(c) \sim \textrm{G}(c, \Lambda)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $c\frac{\beta}{\alpha-1}$ & $\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty of recruitment covered by different models for time having a fixed sample size $c$.}
\label{tab:time_modeling}
\end{table}

\section{Time: Model based on Erlang distribution}
Let $T(c)$ denote the waiting time until $c$ objects are recruited. For a fixed sample size $c$, assuming we have a fixed recruitment rate $\lambda$, $T(c)\sim\textrm{G}(c, \lambda)$. Since $c$ is an integer, we can use the additivity property from the Gamma distribution applicable to independent random variables, $\underbrace{\textrm{Exp} (\lambda) +\cdots +\textrm{Exp} (\lambda)}_{c \ \text{times}} = \textrm{G} (c, \lambda)$. Moreover, this distribution is also called the Erlang distribution and can be denoted as $\textrm{Erlang} (c, \lambda)$. As we can see in Table \ref{tab:time_modeling}, the expectation and variance are the following:

\begin{align*}
\textrm{E}T(c) & = c/\lambda\\
\textrm{Var}(T(c)) & = c/\lambda^2
\end{align*}

The Erlang model explains only the aleatory uncertainty of the waiting time until $c$ objects have been recruited. We can visualize the waiting time in Figure \ref{fig:2_14}.

\section{Time: Derivation of Gamma-Gamma model}

To take into account the epistemic uncertainty of recruitment rates and the aleatory uncertainty of waiting times, a Gamma-Gamma model is recommended \citep{bagiella2001predicting}, $T(c)|\Lambda \sim \textrm{G}(c, \Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. This Gamma-Gamma model has parameters $c$, $\alpha$ and $\beta$. Thus, $T(c)\sim \textrm{GG}(c, \alpha, \beta)$.


\begin{align*}
p(t) &= \int_0^{\infty} p(t|\lambda) p(\lambda) d\lambda\\
&= \int_0^{\infty}\Bigg( \frac{\lambda^ct^{c-1}\exp(-\lambda t)}{\Gamma(c)}\Bigg)\Bigg(\frac{\beta^{\alpha}\lambda^{\alpha-1}\exp(-\beta \lambda)}{\Gamma(\alpha)}\Bigg)d\lambda \\
&=\frac{t^{c-1}\beta^{\alpha}}{\Gamma(c)\Gamma(\alpha)}\int_0^{\infty}\lambda^{c+\alpha-1}\exp(-\lambda(\beta+t))d\lambda\\
&=\frac{\Gamma(\alpha+c)}{\Gamma(c)\Gamma(\alpha)}\frac{t^{c-1}\beta^{\alpha}}{(\beta+t)^{\alpha+c}}\underbrace{\int_0^{\infty}\frac{(\beta+t)^{\alpha+c}}{\Gamma(\alpha+c)}\lambda^{c+\alpha-1}\exp(-\lambda(\beta+t))d\lambda}_{=1}\\
&=\frac{\Gamma(\alpha+c)}{\Gamma(c)\Gamma(\alpha)}\frac{t^{c-1}\beta^{\alpha}}{(\beta+t)^{\alpha+c}}\\
&=\frac{\beta^{\alpha}}{\B(\alpha, c)}\frac{t^{c-1}}{(\beta+t)^{\alpha+c}}
\end{align*}

Thus, $T(c)\sim \textrm{GG} (c, \alpha, \beta)$, see \cite{held2014applied}, where they use a slightly different notation.

Using the expressions of iterated expectation and variance \citep{held2014applied}, the expectation and variance from the respective random variables $T(c)|\Lambda \sim \textrm{G}(c, \Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, and the fact that when $\Lambda \sim \textrm{G}(\alpha, \beta)$ then, $\frac{1}{\Lambda} \sim \textrm{IG}(\alpha, \beta)$ with:

\begin{align*}
\textrm{E}\Bigg [\frac{1}{\Lambda}\Bigg ] &=\textrm{E}(\textrm{IG}(\alpha, \beta)) = \frac{\beta}{\alpha-1}\\
\textrm{Var}\Bigg [\frac{1}{\Lambda}\Bigg ] & =\textrm{Var}(\textrm{IG}(\alpha, \beta)) = \frac{\beta^2}{(\alpha-1)^2(\alpha-2)}
\end{align*}

We have that:

\begin{align*}
\textrm{E}T(c) &= \textrm{E}_{\Lambda}[\textrm{E}_{T(c)} (T(c)|\Lambda)] = \textrm{E}_{\Lambda}\Bigg [\frac{c}{\Lambda}\Bigg ] = c \textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda}\Bigg ] = c\frac{\beta}{\alpha-1}\\
&\alpha>1
\end{align*}


\begin{align*}
\textrm{Var}(T(c)) &= \textrm{Var}_{\Lambda}[\textrm{E}_{T(c)} (T(c)|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_{T(c)}(T(c)|\Lambda)]\\
&=\textrm{Var}_{\Lambda}\Bigg [\frac{c}{\Lambda}\Bigg ] + \textrm{E}_{\Lambda}\Bigg [\frac{c}{\Lambda^2}\Bigg ] \\
&=c^2\textrm{Var}_{\Lambda}\Bigg [\frac{1}{\Lambda}\Bigg ] + c\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ] \\
&=\frac{c^2\beta^2}{(\alpha-1)^2(\alpha-2)} + \frac{c\beta^2}{(\alpha-1)(\alpha-2)}\\
&=\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}\\
&\alpha>2
\end{align*}

As we can see in Table \ref{tab:time_modeling}. Here, we have again, a clear example of overdispersion because the variance is larger than the expectation:

\begin{align*}
\textrm{Var}(T(c))&=\textrm{E}(T(c))\frac{\beta(c+\alpha-1)}{(\alpha-1)(\alpha-2)}\\
&=\textrm{E}(T(c))\beta\Bigg(\frac{c}{(\alpha-1)(\alpha-2)}+\frac{1}{\alpha-2}\Bigg)
\end{align*}

For small parameter $\alpha$ ($\alpha>2$) and large parameter $\beta$, we will have larger uncertainty (variance).

To compute $\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ]$ we use property $\textrm{Var}X = \textrm{E}X^2-(\textrm{E}X)^2$, therefore, $\textrm{E}X^2 = \textrm{Var}X + (\textrm{E}X)^2$. Thus,

\begin{align*}
\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ] &= \textrm{Var}\Bigg [\frac{1}{\Lambda}\Bigg ] + \Bigg [\textrm{E}\frac{1}{\Lambda}\Bigg ]^2\\
&=\frac{\beta^2}{(\alpha-1)^2(\alpha-2)} + \frac{\beta^2}{(\alpha-1)^2}\\
&=\frac{\beta^2(1+\alpha-2)}{(\alpha-1)^2(\alpha-2)}\\
&=\frac{\beta^2}{(\alpha-1)(\alpha-2)}
\end{align*}

\begin{figure}
<<echo = FALSE,  fig.width=7, fig.height=5>>=
par(mfrow=c(1,3))
curve(dgamma(x, shape = 324, rate = 548), 
			from = 0, to = 1,
			main = "Prior", 
			xlab = "Recruitment rate", 
			ylab = "",
			col = "red")

curve(dgamma(x, shape = 32.4, rate = 54.8), add = TRUE,
			col = "blue")

curve(dgamma(x, shape = 3.24, rate = 5.48), add = TRUE,
			col = "green")

legend("topleft",
			 legend = c(expression("G (" ~ alpha == 324 ~ beta == 548 ~ ")"),
			 					 expression("G (" ~ alpha == 32.4 ~ beta == 54.8 ~ ")"),
			 					 expression("G (" ~ alpha == 3.24 ~ beta == 5.48 ~ ")")),
			 col = c("red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)


plot(0:800, dgamma(0:800, shape = 324, rate = 0.591), 
		 type = "l",
		 main = "PMF", 
		 xlab = "Time [days]", 
		 ylab = "", 
		 col = "black")

dgammagamma <- function(t, alpha, b, c) {
	log_dens <- alpha * log(b) - lbeta(alpha, c) + (c - 1) * log(t) - (alpha + c) * log(b + t)
	dens <- exp(log_dens)
	return(dens)
}

pgammagamma <- function(t, alpha, b, c) {
	sapply(t, function(x) {
		integrate(function(u) dgammagamma(u, alpha, b, c), lower = 0, upper = x)$value
	})
}

lines(0:800, dgammagamma(0:800, alpha = 324, b = 548, c = 324), col = "red")
lines(0:800, dgammagamma(0:800, alpha = 32.4, b = 54.8, c = 324), col = "blue")
lines(0:800, dgammagamma(0:800, alpha = 3.24, b = 5.48, c = 324), col = "green")


legend("topleft",
			 legend = c(expression(G(c == 324 ~ lambda == 0.591)),
			 					 expression(GG ~ (alpha == 324 ~ beta == 548)),
			 					 expression(GG ~ (alpha == 32.4 ~ beta == 54.8)),
			 					 expression(GG ~ (alpha == 3.24 ~ beta == 5.48))),
			 col = c("black", "red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)



plot(0:800, pgamma(0:800, shape = 324, rate = 0.591), 
		 type = "l",
		 main = "CDF", 
		 xlab = "Time [days]", 
		 ylab = "", 
		 col = "black")

lines(0:800, pgammagamma(0:800, alpha = 324, b = 548, c = 324), col = "red")
lines(0:800, pgammagamma(0:800, alpha = 32.4, b = 54.8, c = 324), col = "blue")
lines(0:800, pgammagamma(0:800, alpha = 3.24, b = 5.48, c = 324), col = "green")


legend("topleft",
			 legend = c(expression(G(c == 324 ~ lambda == 0.591)),
			 					 expression(GG ~ (alpha == 324 ~ beta == 548)),
			 					 expression(GG ~ (alpha == 32.4 ~ beta == 54.8)),
			 					 expression(GG ~ (alpha == 3.24 ~ beta == 5.48))),
			 col = c("black", "red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)
@
	\caption{Sensitivity analysis between Erlang distribution with $c = 324$ and $\lambda = 0.591$ and Gamma-Gamma model at $c = 324$ changing the parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$, $\textrm{GG}(c=324,\alpha, \beta)$.}
  \label{fig:2_6a}
\end{figure}



\begin{figure}
<<echo=FALSE>>=
set.seed(2025)

M <- n <- 100
Nneed <- Ctarget <- 324
alpha <- 324
t <- seq(1, 550, 1)
lambda <- 0.591

# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
for (i in 1:n) {
  cval <- rpois(length(t), lambda)
  cval_cum_matrix[i, ] <- cumsum(cval)
}

final_counts <- cval_cum_matrix[, length(t)]
tval <- rgamma(n, shape = alpha, rate = lambda)

# instead of tval: timep

timep <- rep(0, M)
csump <- rep(0, M)

for(m in 1:M){
	while (csump[m] < Nneed) {
		csump[m] <- csump[m] + rpois(1, lambda)
		timep[m] <- timep[m] + 1
	}
}

# Define layout: 2 rows, 2 columns (top spans both columns)
layout(matrix(c(1, 1, 2, 3), nrow = 2, byrow = TRUE),
       heights = c(2, 6), widths = c(3, 1))

# --- Plot 1: tval histogram (top, horizontal barplot) ---
par(mar = c(0.1, 25, 2, 10))
tval_bins <- seq(min(tval), max(tval), length.out = 15)
tval_hist <- hist(tval, breaks = tval_bins, plot = FALSE)

barplot(tval_hist$counts,
				space = 0,
				col = "skyblue",
				axes = FALSE,
				horiz = FALSE)

# --- Plot 2: Accrual time series (bottom-left) ---
par(mar = c(4, 4, 2, 1))
plot(t, cval_cum_matrix[1,],
     type = "n",
     main = "Accrual of 100 Studies",
     xlab = "Time",
     ylab = "Cumulative Count",
		 ylim = c(0, 500))

for (i in 1:n) {
  lines(t, cval_cum_matrix[i,], col = "lightgray")
}

lines(t, lambda * t, lwd = 2)
lines(t, qpois(p = 0.975, lambda * t), lty = 2, col = "red")
lines(t, qpois(p = 0.025, lambda * t), lty = 2, col = "red")
abline(h = 324, lty = 5, col = "skyblue", lwd = 2)

legend("topleft",
       legend = c("2.5th - 97.5th Percentile [95%]",
                  "Expected Accrual"),
       col = c("red", "black"),
       lty = c(2, 1),
       lwd = c(1, 2),
       bg = "white",
       cex = 0.8)

# --- Plot 3: Final counts histogram (bottom-right) ---
# par(mar = c(25, 0.5, 2, 1))
# hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
# hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)
#
# barplot(hist_data$counts,
# 				horiz = TRUE,
# 				space = 0,
# 				col = "gray",
# 				axes = FALSE)

@
  \caption{Poisson-distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the red dashed lines indicate Poisson's 95\% aleatory uncertainty. The histogram on the top illustrates the distribution of waiting times to accrue $c=324$ objects when the recruitment rate per unit of time is $\lambda = 0.591$, modelling waiting time with $T(c)\sim \textrm{Erlang} (c=324, \lambda = 0.591)$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_13}
\end{figure}


\begin{figure}
<<echo=FALSE>>=
set.seed(2025)

M <- n <- 100
t <- seq(1, 700, 1)
Nneed <- Ctarget <- 324
lambda <- 0.591

alpha <- 32.4
beta <- 54.8

# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
v_lambda <- rgamma(n, shape = alpha, rate = beta)

for (i in 1:n) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# v_lambda <- rgamma(n, shape = alpha, rate = beta)
# tval <- numeric(n)  
# 
# for (i in 1:n) {
# 	tval[i] <- rgamma(1, shape = alpha, rate = v_lambda[i])
# }
# 
# 
# timepg <- rep(0, M)
# csumpg <- rep(0, M)
# 
# for(m in 1:M){
# 	while (csumpg[m] < Nneed) {
# 		csumpg[m] <- csumpg[m] + rnbinom(1, size = alpha, mu = lambda*t)
# 		timepg[m] <- timepg[m] + 1
# 	}
# }

simulate_time_to_threshold_GG_easier <- function(MM, CC, aa, bb, rseed=265735) {
  set.seed(rseed)
  timegg <- rep(0, MM)
  lambdar <- rgamma(n=MM, shape = aa, rate = bb)
  for (i in 1:MM){
    timegg[i] <- rgamma(n=1, shape = CC, rate = lambdar[i])
  }
  return(timegg)
}

tval <- simulate_time_to_threshold_GG_easier(MM=M, CC=Ctarget, aa=alpha, bb=beta) 


# Define layout: 2 rows, 2 columns (top spans both columns)
layout(matrix(c(1, 1, 2, 3), nrow = 2, byrow = TRUE), 
       heights = c(2, 6), widths = c(3, 1))


# --- Plot 1: tval histogram (top, horizontal barplot) ---
par(mar = c(0.1, 17, 2, 10))
tval_bins <- seq(min(tval), max(tval), length.out = 15)
tval_hist <- hist(tval, breaks = tval_bins, plot = FALSE)

barplot(tval_hist$counts,
				space = 0,
				col = "skyblue",
				axes = FALSE,
				horiz = FALSE)

# --- Plot 2: Accrual time series (bottom-left) ---
par(mar = c(4, 4, 2, 1))
plot(t,  cval_cum_matrix[1,], 
		 type = "n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count",
		 ylim =c(0, 700))

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

lines(t, lambda*t)
lines(t, qnbinom(p = 0.975, size = alpha, mu = lambda*t), lty = 2, col = "green")
lines(t, qnbinom(p = 0.025, size = alpha, mu = lambda*t), lty = 2, col = "green")
abline(h = 324, lty = 5, col = "skyblue", lwd = 2)

legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("green", "black"),
			 lty = c(2, 1),
			 lwd = c(1,2),
			 bg = "white",
			 cex = 0.5)

# # --- Plot 3: Final counts histogram (bottom-right) ---
# par(mar = c(25, 0.5, 2, 1))
# hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
# hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)
# 
# barplot(hist_data$counts,
# 				horiz = TRUE,
# 				space = 0,
# 				col = "gray",
# 				axes = FALSE)
@   
  \caption{Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the green dashed lines indicate Poisson-Gamma 95\% aleatory and epistemic uncertainty. The histogram on the top illustrates the distribution of waiting times to accrue $c=324$ objects when the recruitment rate per unit of time follows $\Lambda \sim \textrm{G}(\alpha = 32.4, \beta = 54.8)$, modelling waiting time with $T(c)\sim \textrm{GG} (c=324, \alpha = 32.4, \beta = 54.8)$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_14}
\end{figure}