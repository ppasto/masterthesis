% LaTeX file for Chapter 02









\chapter{Background} 

\section{Definitions}

The general notion of \textbf{Recruitment} in this Master Thesis refers to the number of patients (Counts) at the Eligibility, or Enrollment, or Randomization, or Statistical Analysis stage in Figure \ref{fig:2_1_a}. Figure \ref{fig:2_1_a} is a schematic representation of a CONSORT study flow chart \citep{schulz2010consort, hopewell2025consort} inspired by a real CONSORT study flow chart in Figure \ref{fig:2_0}. We define \textbf{Accrual} as cumulative recruitment.

The \textbf{Target Population} is a specific group within the broader population, defined by attributes relevant to the research question. This group is focused on criteria that match the study's goals \citep{willie2024population}. Defining the target population allows researchers to refine their objectives and recruitment methods to align with the study's aims.


The \textbf{Eligibility} criteria are the specific requirements that individuals must meet to participate in a study. Eligible patients will be selected from the target population. Inclusion criteria specify the conditions that allow individuals to participate in the trial, particularly focusing on the medical condition of interest. Any other factors that limit eligibility are classified as exclusion criteria \citep{van2007eligibility}, conditions or circumstances that disqualify potential participants \citep{food2018evaluating}.


In clinical trials, \textbf{Enrollment} refers to the formal process of registering participants into a study after they have met all eligibility criteria and provided informed consent. This process includes verifying that each participant satisfies the inclusion and exclusion criteria outlined in the study protocol \citep{NIH2021}. It is important to distinguish between recruitment and enrollment. Recruitment involves identifying and inviting potential participants to join the study, whereas enrollment occurs after these individuals have been screened, consented, and officially registered into the trial \citep{frank2004current}. 

Once enrolled, participants are assigned to specific treatment groups or interventions as defined by the study design. The most common practice is \textbf{Randomization}. In clinical research, randomization is the process of assigning participants to different treatment groups using chance methods, such as random number generators or coin flips \citep{lim2019randomization}. Randomized controlled trials (RCTs) are considered the most effective method for preventing bias in the evaluation of new interventions, drugs, or devices. \citep{van2007eligibility}.


In clinical research, \textbf{Statistical Analysis} involves applying statistical methods to collect, summarize, interpret, and present data derived from clinical studies. This process is essential for evaluating the safety, efficacy, and overall outcomes of medical interventions, ensuring that conclusions drawn are both reliable and valid \citep{panos2023statistical}. Not all participants who are randomized may be included in the final statistical analysis due to protocol deviations of patients not adhering to the protocol \citep{rehman2020exclusion}, missing data \citep{shih2002problems} or loss-to-follow-up, some participants may become unreachable or withdraw consent during the study, resulting in missing outcome data \citep{nuesch2009effects}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_a.png}
  \caption{Patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_a}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{whelan_flow.png}
  \caption{CONSORT study flow-chart \citep{whelan2018high, schulz2010consort, hopewell2025consort}.}
  \label{fig:2_0}
\end{figure}

The number of patients decreases at each stage of a clinical study, from defining the target population to final statistical analysis, see Figure \ref{fig:2_1_b}. Eligibility criteria narrow down participants, and enrollment further reduces numbers as only those meeting strict criteria are registered. Randomization assigns individuals to treatment groups, but some may later be excluded due to protocol deviations, missing data, or loss to follow-up. This process is known as patient leakage \citep{desai2014preventing}, alternative terms are attrition or retention.

Figure \ref{fig:2_1_a} generalizes the notion of patient leakage found in trial profiles such as the one found in Figure \ref{fig:2_0}. Figure \ref{fig:2_1_a} outlines the various stages of a clinical trial and analyzes the key factors contributing to patient attrition as they transition from one stage to the next. Although these stages differ, we apply the same general notion of Recruitment to each stage meaning ``the number of patients (counts) at this stage''.




\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_b.png}
  \caption{Visual representation of patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_b}
\end{figure}

\section{Uncertainty and models for counts and time}

There are two types of uncertainty, aleatory and epistemic \citep{ohagan2006}. The \textbf{Aleatory Uncertainty} reflects randomness that is inherent, irreducible and unpredictable in nature. \textbf{Epistemic Uncertainty} arises primarily from limited or imperfect knowledge about the parameters of a statistical model and can reflect fluctuations of the parameter. Obtaining more or better information about the parameter typically reduces the epistemic uncertainty. 


Let us denote

\begin{itemize}
\item $T=time$
\item $C=counts$
\item $\lambda=\frac{C}{T}$
\end{itemize}

We define \textbf{Design Stage} as the stage where no data is available and elaborated parameters are used in order to determine the prior predictive distributions. We define \textbf{Recruitment Rate} $\lambda=\frac{C}{T}$ at which patients are collected, measured as persons per unit of time, where \textbf{Rate} is understood as a ratio in which the numerator and denominator are incremental differences \citep{piantadosi2024clinical}:

\begin{align*}
\lambda = \frac{\Delta C}{\Delta T} = \frac{C_1 - C_0}{T_1 - T_0} = \frac{C_1 - 0}{T_1 - 0} = \frac{C_1}{T_1}
\end{align*}
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $C = \lambda  $ & $\lambda  $ & 0 & No & No \\
% Poisson & $C \sim \textrm{Po} (\lambda )$ & $\lambda $ & $\lambda  $ & Yes & No \\
% Poisson-Gamma & $C \sim \textrm{Po} (\Lambda )$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha(\beta+1)}{\beta^2}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty of recruitment in one unit of time recruitment covered by different models for counts.}
% \label{tab:count_modeling0}
% \end{table}
% 
% 
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $C(t) = \lambda  t$ & $\lambda  t$ & 0 & No & No \\
% Poisson & $C(t) \sim \textrm{Po} (\lambda  t)$ & $\lambda  t$ & $\lambda  t$ & Yes & No \\
% Poisson-Gamma & $C(t) \sim \textrm{Po} (\Lambda  t)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $t\frac{\alpha}{\beta}$ & $t\frac{\alpha(\beta+t)}{\beta^2}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty in accrual until $t$ covered by different models for counts.}
% \label{tab:count_modeling_20}
% \end{table}
% 
% 
% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
%  \textbf{Methods} & \textbf{Time} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
% \hline
% \hline
% Expectation & $T(c) = c/\lambda$ & $c/\lambda$ & 0 & No & No \\
% Erlang & $T(c) \sim \textrm{G}(c, \lambda) $ & $c/\lambda$ & $c/\lambda^2$ & Yes & No \\
% Gamma-Gamma & $T(c) \sim \textrm{G}(c, \Lambda)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $c\frac{\beta}{\alpha-1}$ & $\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}$ & Yes & Yes \\
% \end{tabular}
% }
% \caption{Moments and aleatory and epistemic uncertainty of recruitment covered by different models for time having a fixed sample size $c$.}
% \label{tab:time_modeling0}
% \end{table}


Where $C_0, C_1$ are the counts at the beginning and end of a study and $T_0, T_1$, the waiting times. At the design stage $C_0=T_0=0$. Methods in Tables \ref{tab:count_modeling}, \ref{tab:count_modeling_2} and \ref{tab:time_modeling} are applicable to each level of recruitment in Figures \ref{fig:2_1_a} and \ref{fig:2_1_b}.

\chapter{Methods for Counts} 
\section{Counts: Model based on Expectations}
\label{sec:expect}

If we fix the duration of a study at time $T$ and we expect that we collect $C$ patients until $T$, we deterministically predict the recruitment rate per one unit of time (without taking into consideration any uncertainty) to be $\hat{\lambda}=\frac{C}{T}$ \citep{carter2004application}. 




\subsection{Expected recruitment in one unit of time}

Assume that the recruitment in one unit of time is constant and equal to $\lambda$. Then, $C = \textrm{E}C = \textrm{E}\lambda = \lambda$ and $\textrm{Var}(C) = \textrm{Var}(\lambda) = 0$, as we can see in Table \ref{tab:count_modeling}.

\subsection{Expected accrual at time point $t$}

Assuming that recruitments in each unit of time are independent of each other we have:

$C(t) = \textrm{E}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = \textrm{E}(\lambda t) = \lambda t$\\
$\textrm{Var}(C(t)) = \textrm{Var}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = t \textrm{Var}(\lambda) = 0$ or $\textrm{Var}(C(t))=\textrm{Var}(tC)=t^2\textrm{Var}(C)=0$

Both the expected accrual and its zero-variance are recorded in Table \ref{tab:count_modeling_2}
and visualized in Figure \ref{fig:2_2} and Figure \ref{fig:2_5}.

\subsection{Criticism}

This is a simple, widely-used deterministic method based on a linear extrapolation of the constant expected recruitment also called "First Order Recruitment Model" (FORM) \citep{comfort2013}. It is also referred to as the \textit{unconditional approach} and its main limitation is the lack of a mechanism to account for known sources of variation in the rate \citep{carter2005practical}. 

The model based on expectations is overly simplistic and fails to account for changes in center recruitment or the regulatory environment \citep{barnard2010systematic}. Therefore, stochastic models that incorporate randomness in the recruitment process are more suitable than this simple deterministic approach \citep{zhang2012modeling}.



\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C = \lambda  $ & $\lambda  $ & 0 & No & No \\
Poisson & $C \sim \textrm{Po} (\lambda )$ & $\lambda $ & $\lambda  $ & Yes & No \\
Poisson-Gamma & $C \sim \textrm{Po} (\Lambda )$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha(\beta+1)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty of recruitment in one unit of time recruitment covered by different models for counts.}
\label{tab:count_modeling}
\end{table}




\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C(t) = \lambda  t$ & $\lambda  t$ & 0 & No & No \\
Poisson & $C(t) \sim \textrm{Po} (\lambda  t)$ & $\lambda  t$ & $\lambda  t$ & Yes & No \\
Poisson-Gamma & $C(t) \sim \textrm{Po} (\Lambda  t)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $t\frac{\alpha}{\beta}$ & $t\frac{\alpha(\beta+t)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty in accrual until $t$ covered by different models for counts.}
\label{tab:count_modeling_2}
\end{table}


\section{Counts: Model based on Poisson Process}

One way to incorporate variation in the mean number of participants per day is by assuming that participants are recruited according to a known probability distribution, such as the Poisson distribution \citep{carter2004application}. This approach emulates trial accrual using a random number generator and records the numbers of patients (counts) recruited at each time point over multiple iterations \citep{carter2005practical}. 

The resulting distribution helps estimate the probability of completing accrual within a given time frame and assess variability in accrual. This method is particularly useful when a trial has a fixed duration, as it allows researchers to determine the necessary number of clinical centers and monthly recruitment rate to achieve a high probability (e.g., 80\%) of completing enrollment on time \citep{carter2005practical}.

The Poisson distribution $C\sim \rm{Po} (\lambda)$ allows us to explain the recruitment of patients. It is a discrete variable that expresses the probability of a given number of events (in our case, patient recruitment) occurring in a fixed unit interval of time. We assume that these events occur with a known constant rate $\lambda$ and are independent between units of time.

\begin{align*}
\textrm{P}[C=&c] = \frac{\lambda^c}{c!}e^{-\lambda}, \ c = 0,1,2,\ldots
\end{align*}


One important property from the Poisson distribution is that it is infinitely divisible \citep{held2014applied}. If $X_i\sim \textrm{Po} (\lambda_i)$ for $i=1,\ldots, n$ are independent, then, $\sum_{i=1}^n X_i \sim \textrm{Po} \Big( \sum_{i=1}^n \lambda_i \Big)$.

\subsection{Recruitment in one unit of time}

The recruitment of patients in one unit of time follows $C\sim \textrm{Po} (\lambda)$ and the expectation and variance are
$\textrm{E}C = \lambda$ and $\textrm{Var}(C) = \lambda$, as we can see in Table \ref{tab:count_modeling}

\subsection{Accrual at time point $t$}
At time point $t$, the accrual follows $C\sim \textrm{Po} (\lambda t)$. Using the infinitely divisible property from the Poisson applicable to independent random variables, $\underbrace{\textrm{Po} (\lambda) +\cdots +\textrm{Po} (\lambda)}_{t \ \text{times}} = \textrm{Po} (\lambda t)$. We assume that the recruitment of patients in $t$ unit time intervals is independent from another. As we can see in Table \ref{tab:count_modeling_2}, the expectation and variance are the following:

\begin{align*}
\textrm{E}C(t) & = \lambda t \\
\textrm{Var}(C(t)) & = \lambda t
\end{align*}

For example, if we assume $\lambda = 0.591$ per day and $t=550$, we can show the accrual of 100 different studies in Figure \ref{fig:2_2} and the histogram at $t=550$ days. The exact distribution at $t=550$ is provided in Figure \ref{fig:2_3} and the Cummulative Distribution Function (CDF) in Figure \ref{fig:2_4}. The uncertainty bands based on the exact quantiles are displayed in Figure \ref{fig:2_5}.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-3-1} 

}


\end{knitrout}
  \caption{Poisson-distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the red dashed lines indicate Poisson's 95\% aleatory uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_2}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-4-1} 

}


\end{knitrout}
  \caption{Probability Mass Function (PMF) of Poisson-distributed counts: This bar plot represents the probability mass function (PMF) of counts ranging from 200 to 500, using a Poisson distribution $\textrm{Po}(\lambda t)$ with a rate parameter $\lambda = 0.591$ per day at time $t = 550$ days.}
  \label{fig:2_3}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-5-1} 

}


\end{knitrout}
  \caption{Cumulative Distribution Function (CDF) of Poisson-distributed counts: The bar plot illustrates the cumulative probability distribution for counts within the range of 200 to 500, using a Poisson $\textrm{Po}(\lambda t)$ distribution with a rate parameter $\lambda = 0.591$ per day at time $t=550$ days.}
  \label{fig:2_4}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-6-1} 

}


\end{knitrout}
  \caption{Predicted uncertainty bands for Poisson process with $\lambda = 0.591$ per day. The black line represents the expected accrual, while the red shaded regions indicate aleatory uncertainty: the dark red band spans the interquantile range (25th - 75th percentiles), the lighter red band cover the 10th - 90th percentile range and the light red the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_5}
\end{figure}

\subsection{Criticism}

This model was used by \cite{carter2004application} and \cite{carter2005practical}. Although this model accounts for aleatory uncertainty, the recruitment rate is assumed to be constant for the entire period of time. Therefore, an alternative method that accounts for varying recruitment rates over time is necessary.

\section{Counts: Negative Binomial model derived from Poisson-Gamma model}

The basic Poisson model does not account for variations in recruitment rates or uncertainties in rate estimates \citep{mountain2022recruitment}. To address this issue, \cite{anisimov2007modelling} propose a random effects model where recruitment follows a homogeneous Poisson process with rates drawn from a gamma distribution, a probabilistic model with Poisson-Gamma mixture distribution. In this approach, rates are treated as random variables with a prior gamma distribution, whose parameters are either elicited from historical data or estimated using current recruitment data. This allows for a posterior distribution of rates to be used for recruitment prediction in a Bayesian or an empirical Bayesian framework.  

Building on this, a Poisson-Gamma model can be implemented by randomly generating recruitment rates using a Gamma distribution and plugging them into a Poisson process. This model can be used to make two types of projections: a point prediction estimating when the expected number of events will reach a specific sample size and a Bayesian interval prediction based on the generation of future accrual and event dates. This method enables flexible forecasting for any milestone at any calendar time \citep{bagiella2001predicting}, as we will see in Chapter 4.


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-7-1} 

}


\end{knitrout}
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Poisson-Gamma changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=1$.}
  \label{fig:2_6}
\end{figure}

\subsection{Recruitment in one unit of time}

Let $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. Where $\Lambda$ represents the "recruitment proneness" in the unit of time. The prior predictive distribution for this Poisson-Gamma model can be computed as follows:

\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{\lambda^c\exp(-\lambda)}{c!}\Bigg[\lambda^{\alpha-1}\exp(-\beta\lambda)\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c)}{c!\Gamma(\alpha) (\beta+1)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+1)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+1)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots
\end{align*}

Thus, $C\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+1}\Bigg)$.


The parameter $\lambda$ in the integral represents the expected recruitment rate per unit of time for an individual and this recruitment rate is assumed to vary from individual to individual as it is generated by a $\textrm{G}(\alpha,\beta)$ distribution \citep{johnson2005univariate}.


Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}C &= \textrm{E}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda] = \alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_C(C|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda] + \textrm{E}_{\Lambda}[\Lambda] \\
&=\alpha/\beta^2 + \alpha/\beta = \frac{\alpha(\beta+1)}{\beta^2}
\end{align*}


There are two different interpretations of the Negative Binomial distribution, Failure-Based and Count-based.
\subsection{Failure-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}(r,p)$ models the number of \textbf{failures} before achieving a fixed number of \textbf{successes} in a sequence of Bernoulli trials. 
\item Parametrization:
	\begin{itemize}
	\item $r$: Number of successes to be achieved (fixed).
	\item $p$: Probability of success in each trial
	\item The random variable $X$ represents the number of failures before achieving $r$ successes.
	\end{itemize}
\item Probability Mass Function (PMF) \citep{R-stats}:
\begin{align*}
\textrm{P}(X=& x) = \frac{\Gamma(x+r)}{\Gamma(x) x!}  p^r(1-p)^x, \\
&x = 0,1,2, \ldots, r >0\\
&0<p\leq 1
\end{align*}
where $x$ is the number of failures.
\item Interpretation: In a sequence of independent binary trials with constant probability $p$ of observing a \textit{recruited} patient, $r$ is the number of \textit{recruited} patients observed at the time that $x$ \textit{non-recruited} patients are observed \citep{meeker2017statistical}.


\end{enumerate}

With respect to the parameters, $r>0$ represents the number of successes until 
the experiment is stopped. The success probability in each experiment is 
represented by $p\in[0,1]$.  In R the functions \_\texttt{nbinom(..., size = $n$, prob = $p$)} relate to the random variable $X-r$, the number of failures (as opposed to the number of trials) until $r$ successes have been achieved \citep{held2014applied}. Regarding our prior interpretation, in our application, a success is a recruited patient whereas a failure is a non-recruited patient.

\begin{align*}
EX & = \frac{r(1-p)}{p}\\
Var(X) & = \frac{r(1-p)}{p^2}
\end{align*}

Since we will be using the Count-Based interpretation of the Negative Binomial \citep{hilbe2011negative}, our parametrization relates to R with $r = \alpha$ and $p = \frac{\beta}{\beta+1}$.

\subsection{Count-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}\Bigg(\alpha,\frac{\beta}{\beta+1}\Bigg)$ can also be seen as a Poisson-Gamma mixture, where the observed count data follows a Poisson distribution with a mean that itself follows a Gamma distribution, $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. 
\item Parametrization:
	\begin{itemize}
	\item $\mu = \frac{\alpha}{\beta}$: Mean of the distribution (expected number of occurrences).
	\item $\alpha$: Dispersion parameter, controlling the variance.
	\end{itemize}
\item Alternative formulation of the PMF \citep{hilbe2011negative}:
\begin{align*}
\textrm{P}(X=&c) = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\beta+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha}, \ c\geq 0 
\end{align*}
where $c$ are the counts.
\item Interpretation: This model is used to represent "recruitment proneness". The parameter $\mu$ represents the expected number of recruitments in a study \citep{johnson2005univariate}.

\end{enumerate}

In the count-based method we take into account the overdispersion of the data. When the variability in the observed data is greater than what is expected.

\begin{align*}
\textrm{E}X&=\mu = \frac{\alpha}{\beta}\\
\textrm{Var}X&=\mu + \frac{\mu}{\beta}
\end{align*}

How do we get to our formulation of the PMF:

\begin{align*}
\textrm{P}(X=c) & = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\alpha+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha}{\alpha\beta+\alpha}\Bigg)^{c} \Bigg(\frac{\beta\alpha}{\alpha\beta+\alpha}\Bigg)^{\alpha} \\
&= \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}
\end{align*}



% 
% \begin{align*}
% Mean &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha(\beta+1)}{\beta(\beta+1)}\\
% &= \frac{\alpha}{\beta}
% \end{align*}
% 
% \begin{align*}
% Variance &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha(\beta+1)^2}{\beta^2(\beta+1)}\\
% &= \frac{\alpha(\beta+1)}{\beta^2}
% \end{align*}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-8-1} 

}


\end{knitrout}
  \caption{Poisson-Gamma $(\alpha = 324, \beta = 548)$ distributed counts with $\mu = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the red dashed lines indicate Poisson-Gamma 95\% aleatory and epistemic uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_7}
\end{figure}



\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-9-1} 

}


\end{knitrout}
  \caption{Predicted uncertainty bands for Poisson-Gamma process with $\mu = 0.591$ per day. The black line represents the expected accrual, while the green shaded regions indicate aleatory and epistemic uncertainty: the dark green band spans the interquantile range (25th - 75th percentiles), the lighter green band cover the 10th - 90th percentile range and the light green the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_8}
\end{figure}

\subsection{Sensitivity Analysis}

In Figures \ref{fig:2_6b} and \ref{fig:2_6}, the Poisson distribution captures aleatory uncertainty, while the Gamma prior represents epistemic uncertainty. The Poisson-Gamma distribution incorporates both types of uncertainty. The sensitivity analysis, also shown in Figures \ref{fig:2_6b} and \ref{fig:2_6}, highlights that while the expectation remains unchanged, smaller parameter values lead to greater overall uncertainty due to the increased variance introduced by the Gamma distribution. The smaller the $\beta$, the greater the variance.


\begin{align*}
\textrm{Var}(\textrm{G}(\alpha, \beta)) = \frac{\alpha}{\beta^2} =\frac{\textrm{E}(\textrm{G}(\alpha, \beta))}{\beta}
\end{align*}

In the Poisson-Gamma model we can interpret $\alpha$ as the parameter that represents the sample size and $\beta$ represents time.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-10-1} 

}


\end{knitrout}
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Poisson-Gamma changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=550$.}
  \label{fig:2_6b}
\end{figure}

\subsection{Accrual at time point $t$}

Let $C(t)|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. By the infinitely divisible property of the Poisson distribution we have that $\underbrace{\textrm{Po} (\Lambda) +\cdots +\textrm{Po} (\Lambda)}_{t \ \text{times}} = \textrm{Po} (\Lambda t)$. This Poisson-Gamma model has parameters $t$, $\alpha$ and $\beta$. Hence, we denote it as $C(t)\sim \textrm{PoG}(t, \alpha, \beta)$. The prior predictive distribution can be computed as follows:

\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda t) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{(\lambda t)^c\exp(-\lambda t)}{c!}\Bigg[(\lambda)^{\alpha-1}\exp(-\beta\lambda )\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha t^c}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda t)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c) t^c}{c!\Gamma(\alpha) (\beta+t)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+t)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+t)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha t^c\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+t}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{t}{\beta+t}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+t}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots, \ \alpha \in \mathbb{N}
\end{align*}
Thus, $C(t)\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+t}\Bigg)$

We will be focusing on the count-based interpretation of the Negative Binomial distribution. We can relate this interpretation to the more popular failure-based interpretation by seeing $\alpha$ as $n$, the number of successes. In our case, the number of patients we wish to recruit. And, $p = \frac{\beta}{\beta+t}$ which is the probability of success, as the probability of recruiting.


Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C(t)|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}(C(t)) &= \textrm{E}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda t] = t\alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C(t)) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_{C(t)}(C(t)|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda t] + \textrm{E}_{\Lambda}[\Lambda t] \\
&=t^2\alpha/\beta^2 + t\alpha/\beta = \frac{t\alpha(\beta+t)}{\beta^2}
\end{align*}

Therefore, we clearly have overdispersion $\textrm{Var}(C(t))>\textrm{E}(C(t))$. This can be easily seen because:

\begin{align*}
\textrm{Var}(C(t)) &=\textrm{E}(C(t))\frac{\beta+t}{\beta}\\
&=\textrm{E}(C(t))\Bigg ( 1+\frac{t}{\beta} \Bigg)
\end{align*}

\section{Comparison of models for the accrual of counts}

As we can see in Figures \ref{fig:2_10} -- \ref{fig:2_11a}, we are taking into account more epistemic uncertainty of the recruitment rate $\Lambda$ in the Poisson-Gamma model than we do in the Poisson process where $\lambda$ is fixed. Hence, the color-coding, red for only aleatory and green, aleatory and epistemic. In reality we expect fluctuations of recruitment rates, therefore, the Poisson-Gamma model is more realistic.

Figures \ref{fig:2_10} and \ref{fig:2_11} assume that the recruitment rates do not vary much by assuming $\textrm{G}(\alpha = 324, \beta = 548)$ as prior, where the expectation is $\frac{\alpha}{\beta} = 0.591$ and the standard deviation $\frac{\sqrt{\alpha}}{\beta} = 0.033$. In contrast, Figures \ref{fig:2_10a} and \ref{fig:2_11a}, assume a less informative prior $\textrm{G}(\alpha = 32.4, \beta = 54.8)$ of the recruitment rates with expectation $0.591$ and standard deviation $\frac{\sqrt{\alpha}}{\beta} = 0.104$, showing a more pronounced discrepancy between the Poisson and the Poisson-Gamma process.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-11-1} 

}


\end{knitrout}
  \caption{Comparison of the 95\% uncertainty range taken into consideration in the Poisson-Gamma $(\alpha = 324, \beta = 548)$ model as opposed to the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_10}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-12-1} 

}


\end{knitrout}
  \caption{Comparison of the 95\% uncertainty range taken into consideration in the Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ model as opposed to the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_10a}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-13-1} 

}


\end{knitrout}
  \caption{Comparison of the theoretical quantiles in the Poisson-Gamma $(\alpha = 324, \beta = 548)$ model and the 95\% uncertainty range of the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_11}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-14-1} 

}


\end{knitrout}
  \caption{Comparison of the theoretical quantiles in the Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ model and the 95\% uncertainty range of the Poisson $\lambda = 0.591$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_11a}
\end{figure}

\section{Generation of Poisson-Gamma model}

There are two ways with which we can generate the random $\Lambda$ in the $\textrm{PoG}(t, \alpha, \beta)$ model and we will investigate in this section whether they are both equivalent.

\subsection{Version 1 (random-constant)}

We generate a random vector of $\lambda$ values of length $M=10^5$ with $\textrm{G}(\alpha, \beta)$ and a vector of counts at each time point, of length $t$ with $\textrm{Po}(\lambda)$. For the final counts we sum cumulatively (cumsum) the counts at $t$ so that we get the accrual of patients at time $t$ of each study.

\begin{align*}
\lambda^m &\sim \textrm{G}(\alpha, \beta) \\
C(t)^m &\sim \textrm{Po} (\lambda^m t) = \underbrace{\textrm{Po} (\lambda^m) +\cdots +\textrm{Po} (\lambda^m)}_{t \ \text{times}}\\
& m = 1, \ldots, M
\end{align*}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{verbatim}
set.seed(2025)

M <- 10^4
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 32.4
beta <- 54.8

cval_cum_matrix <- matrix(NA, nrow = M, ncol = length(t))
v_lambda <- rgamma(M, shape = alpha, rate = beta)

for (i in 1:M) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)
}
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Version 2 (random-random)}

At each time point $i=1,\ldots, t$, we generate a $\lambda^{(i)}$ with $\textrm{G}(\alpha, \beta)$ and a count with $\textrm{Po}(\lambda^{(i)})$. For the final counts we sum cumulatively (cumsum) the counts at $t$ so that we get the accrual of patients at time $t$ of each study.

\begin{align*}
\lambda^{(i)m} &\sim \textrm{G}(\alpha, \beta) \\
C (i)^m&\sim \textrm{Po} (\lambda^{(i)m})\\
C(t)^m &= \sum_{i=1}^tC(i)^m \\
& i = 1, \ldots, t \\
& m = 1, \ldots, M 
\end{align*}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{verbatim}
set.seed(2025)

M <- 10^4
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 32.4
beta <- 54.8

cval_cum_matrix_2 <- matrix(NA, nrow = M, ncol = length(t))
cval <- rep(NA, length(t))

for (i in 1:M) {
	for(j in 1:length(t)){
		cval[j] <- rpois(1, lambda = rgamma(1, shape = alpha, rate = beta))
	}
	cval_cum_matrix_2[i, ] <- cumsum(cval)
}
\end{verbatim}
\end{kframe}
\end{knitrout}


As we can see in Figure \ref{fig:2_12}, the density of these two versions do not overlap and therefore we can empirically conclude that these two ways of simulating the Poisson-Gamma model are not equivalent. Both methods share the same expectation but the spread differs. We can see these both in Figure \ref{fig:2_12} and theoretically. In Version 1, we have a $C(t)|\Lambda\sim\textrm{Po}(\Lambda t)$ with $\Lambda\sim\textrm{G}(\alpha,\beta)$ with expectation and variance (proved above):


\begin{align*}
\textrm{E}(C(t))=\frac{t\alpha}{\beta}
\end{align*}

\begin{align*}
\textrm{Var}(C(t))=\frac{t\alpha(\beta+t)}{\beta^2}
\end{align*}

Whereas in Version 2, we have a $C(t)|\Lambda^*\sim\textrm{Po}(\Lambda^*)$ with $\Lambda^*\sim\textrm{G}(t\alpha,\beta)$ because:

\begin{align*}
\sum_{i=1}^t C_i &= \sum_{i=1}^t \textrm{Po}(\lambda_i)\\
&=\textrm{Po}\Bigg(\sum_{i=1}^t \lambda_i\Bigg)\\
&=\textrm{Po}\Bigg(\sum_{i=1}^t \textrm{G}(\alpha,\beta)\Bigg)\\
&=\textrm{Po}(\textrm{G}(t\alpha,\beta))
\end{align*}

with,
\begin{align*}
\textrm{E}(C(t))=\frac{t\alpha}{\beta}
\end{align*}

and using the variance iterated formula \citep{held2014applied},

\begin{align*}
\textrm{Var}(C(t))&= \textrm{Var}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda^*] + \textrm{E}_{\Lambda^*}[\textrm{Var}_{C(t)}(C(t)|\Lambda^*)]\\
&=\frac{t\alpha}{\beta}+\frac{t\alpha}{\beta^2}\\
&=\frac{t\alpha(\beta+1)}{\beta^2}
\end{align*}

Hence, the spread in Version 2 is smaller than in Version 1. Version 1 for Counts will be also used in Chapter 5 for Time.


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-15-1} 

}


\end{knitrout}
  \caption{Comparison of the two versions with which we can generate a Poisson-Gamma model. Version 1: generates for each $m=1, \ldots, M$, a vector of $\lambda$ with length $M=10^4$ using a $\textrm{G}(\alpha = 32.4, \beta = 54.8)$ and then uses each of these random $\lambda^m$s to generate $t$ independent $\textrm{Po}(\lambda^m)$ counts. Version 2: at each time point a different random $\lambda$ is generated from $\textrm{G}(\alpha = 32.4, \beta = 54.8)$. We compare both these versions at accrual of time $t=550$ for $\lambda=0.591$.}
  \label{fig:2_12}
\end{figure}



\chapter{Methods for Waiting Time} 
\section{Time: Model based on Expectations}

If we fix the sample size of a study at $c$ and we expect the recruitment rate to be $\lambda$, we deterministically predict the time planned for the study (without taking into consideration any uncertainty) to be $\hat{T}=\frac{c}{\lambda}$ \citep{bagiella2001predicting}. Researchers tend to use simple unconditional methods \citep{white2015projection}. Regarding the expectation and variance in this framework: $\textrm{E}T = \textrm{E}(c/\lambda) = c/\lambda$ and $\textrm{Var}(T) = \textrm{Var}(c/\lambda) = 0$, as we can see in Table \ref{tab:time_modeling}.


\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Time} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $T(c) = c/\lambda$ & $c/\lambda$ & 0 & No & No \\
Erlang & $T(c) \sim \textrm{G}(c, \lambda) $ & $c/\lambda$ & $c/\lambda^2$ & Yes & No \\
Gamma-Gamma & $T(c) \sim \textrm{G}(c, \Lambda)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $c\frac{\beta}{\alpha-1}$ & $\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty of recruitment covered by different models for time having a fixed sample size $c$.}
\label{tab:time_modeling}
\end{table}

\section{Time: Model based on Erlang distribution}
Let $T(c)$ denote the waiting time until $c$ objects are recruited. For a fixed sample size $c$, assuming we have a fixed recruitment rate $\lambda$, $T(c)\sim\textrm{G}(c, \lambda)$. Since $c$ is an integer, we can use the additivity property from the Gamma distribution applicable to independent random variables, $\underbrace{\textrm{Exp} (\lambda) +\cdots +\textrm{Exp} (\lambda)}_{c \ \text{times}} = \textrm{G} (c, \lambda)$. Moreover, this distribution is also called the Erlang distribution and can be denoted as $\textrm{Erlang} (c, \lambda)$. As we can see in Table \ref{tab:time_modeling}, the expectation and variance are the following:

\begin{align*}
\textrm{E}T(c) & = c/\lambda\\
\textrm{Var}(T(c)) & = c/\lambda^2
\end{align*}

The Erlang model explains only the aleatory uncertainty of the waiting time until $c$ objects have been recruited. We can visualize the waiting time in Figures \ref{fig:2_13} and \ref{fig:2_14}.

\section{Time: Derivation of Gamma-Gamma model}

To take into account the epistemic uncertainty of recruitment rates and the aleatory uncertainty of waiting times, a Gamma-Gamma model is recommended \citep{bagiella2001predicting}, $T(c)|\Lambda \sim \textrm{G}(c, \Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. This Gamma-Gamma model has parameters $c$, $\alpha$ and $\beta$. Thus, $T(c)\sim \textrm{GG}(c, \alpha, \beta)$. The prior predictive distribution for the waiting time can be computed as follows:


\begin{align*}
p(t) &= \int_0^{\infty} p(t|\lambda) p(\lambda) d\lambda\\
&= \int_0^{\infty}\Bigg( \frac{\lambda^ct^{c-1}\exp(-\lambda t)}{\Gamma(c)}\Bigg)\Bigg(\frac{\beta^{\alpha}\lambda^{\alpha-1}\exp(-\beta \lambda)}{\Gamma(\alpha)}\Bigg)d\lambda \\
&=\frac{t^{c-1}\beta^{\alpha}}{\Gamma(c)\Gamma(\alpha)}\int_0^{\infty}\lambda^{c+\alpha-1}\exp(-\lambda(\beta+t))d\lambda\\
&=\frac{\Gamma(\alpha+c)}{\Gamma(c)\Gamma(\alpha)}\frac{t^{c-1}\beta^{\alpha}}{(\beta+t)^{\alpha+c}}\underbrace{\int_0^{\infty}\frac{(\beta+t)^{\alpha+c}}{\Gamma(\alpha+c)}\lambda^{c+\alpha-1}\exp(-\lambda(\beta+t))d\lambda}_{=1}\\
&=\frac{\Gamma(\alpha+c)}{\Gamma(c)\Gamma(\alpha)}\frac{t^{c-1}\beta^{\alpha}}{(\beta+t)^{\alpha+c}}\\
&=\frac{\beta^{\alpha}}{B(\alpha, c)}\frac{t^{c-1}}{(\beta+t)^{\alpha+c}}, \ where \ B(\alpha, c) = \frac{\Gamma(c)\Gamma(\alpha)}{\Gamma(\alpha+c)}
\end{align*}

Thus, $T(c)\sim \textrm{GG} (c, \alpha, \beta)$, see \cite{held2014applied}, where they use a slightly different notation.

Using the expressions of iterated expectation and variance \citep{held2014applied}, the expectation and variance from the respective random variables $T(c)|\Lambda \sim \textrm{G}(c, \Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, and the fact that when $\Lambda \sim \textrm{G}(\alpha, \beta)$ then, $\frac{1}{\Lambda} \sim \textrm{IG}(\alpha, \beta)$ with:

\begin{align*}
\textrm{E}\Bigg [\frac{1}{\Lambda}\Bigg ] &=\textrm{E}(\textrm{IG}(\alpha, \beta)) = \frac{\beta}{\alpha-1}\\
\textrm{Var}\Bigg [\frac{1}{\Lambda}\Bigg ] & =\textrm{Var}(\textrm{IG}(\alpha, \beta)) = \frac{\beta^2}{(\alpha-1)^2(\alpha-2)}
\end{align*}

We have that:

\begin{align*}
\textrm{E}T(c) &= \textrm{E}_{\Lambda}[\textrm{E}_{T(c)} (T(c)|\Lambda)] = \textrm{E}_{\Lambda}\Bigg [\frac{c}{\Lambda}\Bigg ] = c \textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda}\Bigg ] = c\frac{\beta}{\alpha-1}, \ \alpha>1
\end{align*}


\begin{align*}
\textrm{Var}(T(c)) &= \textrm{Var}_{\Lambda}[\textrm{E}_{T(c)} (T(c)|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_{T(c)}(T(c)|\Lambda)]\\
&=\textrm{Var}_{\Lambda}\Bigg [\frac{c}{\Lambda}\Bigg ] + \textrm{E}_{\Lambda}\Bigg [\frac{c}{\Lambda^2}\Bigg ] \\
&=c^2\textrm{Var}_{\Lambda}\Bigg [\frac{1}{\Lambda}\Bigg ] + c\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ] \\
&=\frac{c^2\beta^2}{(\alpha-1)^2(\alpha-2)} + \frac{c\beta^2}{(\alpha-1)(\alpha-2)}\\
&=\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}, \ \alpha>2
\end{align*}

As we can see in Table \ref{tab:time_modeling}. Here, we have again, a clear example of overdispersion because the variance is larger than the expectation:

\begin{align*}
\textrm{Var}(T(c))&=\textrm{E}(T(c))\frac{\beta(c+\alpha-1)}{(\alpha-1)(\alpha-2)}\\
&=\textrm{E}(T(c))\beta\Bigg(\frac{c}{(\alpha-1)(\alpha-2)}+\frac{1}{\alpha-2}\Bigg)
\end{align*}

For small parameter $\alpha$ ($\alpha>2$) and large parameter $\beta$, we will have larger uncertainty (variance).

To compute $\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ]$ we use property $\textrm{Var}X = \textrm{E}X^2-(\textrm{E}X)^2$, therefore, $\textrm{E}X^2 = \textrm{Var}X + (\textrm{E}X)^2$. Thus,

\begin{align*}
\textrm{E}_{\Lambda}\Bigg [\frac{1}{\Lambda^2}\Bigg ] &= \textrm{Var}\Bigg [\frac{1}{\Lambda}\Bigg ] + \Bigg [\textrm{E}\frac{1}{\Lambda}\Bigg ]^2\\
&=\frac{\beta^2}{(\alpha-1)^2(\alpha-2)} + \frac{\beta^2}{(\alpha-1)^2}\\
&=\frac{\beta^2(1+\alpha-2)}{(\alpha-1)^2(\alpha-2)}\\
&=\frac{\beta^2}{(\alpha-1)(\alpha-2)}
\end{align*}

For Version 2 of generating $\Lambda^*$ with $T(c)|\Lambda^*\sim\textrm{G}(c,\Lambda^*)$ and $\Lambda^*\sim \textrm{G}(t\alpha,\beta)$, we have as expectation and variance:


\begin{align*}
\textrm{E}T(c) &= \textrm{E}_{\Lambda^*}[\textrm{E}_{T(c)} (T(c)|\Lambda^*)] = \textrm{E}_{\Lambda^*}\Bigg [\frac{c}{\Lambda^*}\Bigg ] = c \textrm{E}_{\Lambda^*}\Bigg [\frac{1}{\Lambda^*}\Bigg ] = c\frac{\beta}{t\alpha-1}, \ t\alpha>1
\end{align*}

\begin{align*}
\textrm{Var}(T(c)) &= \textrm{Var}_{\Lambda^*}[\textrm{E}_{T(c)} (T(c)|\Lambda^*)] + \textrm{E}_{\Lambda^*}[\textrm{Var}_{T(c)}(T(c)|\Lambda^*)]\\
&=\textrm{Var}_{\Lambda^*}\Bigg [\frac{c}{\Lambda^*}\Bigg ] + \textrm{E}_{\Lambda^*}\Bigg [\frac{c}{\Lambda^{*2}}\Bigg ] \\
&=c^2\textrm{Var}_{\Lambda^*}\Bigg [\frac{1}{\Lambda^*}\Bigg ] + c\textrm{E}_{\Lambda^*}\Bigg [\frac{1}{\Lambda^{*2}}\Bigg ] \\
&=\frac{c^2\beta^2}{(t\alpha-1)^2(t\alpha-2)} + \frac{c\beta^2}{(t\alpha-1)(t\alpha-2)}\\
&=\frac{c\beta^2(c+t\alpha-1)}{(t\alpha-1)^2(t\alpha-2)}, \ t\alpha>2
\end{align*}


For $t>1$ this variance from Version 2 is smaller than $\frac{c\beta^2(c+\alpha-1)}{(\alpha-1)^2(\alpha-2)}$, when accrual follows Version 1. We get a narrower distribution if Version 2 is applicable. In Chapter 5, we consider only Version 1 with a higher spread as \cite{carter2004application} suggested (constant $\lambda$ over time).




\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-16-1} 

}


\end{knitrout}
	\caption{Sensitivity analysis between Erlang distribution with $c = 324$ and $\lambda = 0.591$ and Gamma-Gamma model at $c = 324$ changing the parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$, $\textrm{GG}(c=324,\alpha, \beta)$.}
  \label{fig:2_6a}
\end{figure}



\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-17-1} 

}


\end{knitrout}
  \caption{Poisson-distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the red dashed lines indicate Poisson's 95\% aleatory uncertainty. The histogram on the top illustrates the distribution of waiting times to accrue $c=324$ objects when the recruitment rate per unit of time is $\lambda = 0.591$, modelling waiting time with $T(c)\sim \textrm{Erlang} (c=324, \lambda = 0.591)$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_13}
\end{figure}


\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch02_figunnamed-chunk-18-1} 

}


\end{knitrout}
  \caption{Poisson-Gamma $(\alpha = 32.4, \beta = 54.8)$ distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from Section \ref{sec:expect}, while the green dashed lines indicate Poisson-Gamma 95\% aleatory and epistemic uncertainty. The histogram on the top illustrates the distribution of waiting times to accrue $c=324$ objects when the recruitment rate per unit of time follows $\Lambda \sim \textrm{G}(\alpha = 32.4, \beta = 54.8)$, modelling waiting time with $T(c)\sim \textrm{GG} (c=324, \alpha = 32.4, \beta = 54.8)$ \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_14}
\end{figure}
