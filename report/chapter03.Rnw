% LaTeX file for Chapter 03
<<'preamble03',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch03_fig', 
    self.contained=FALSE,
    cache=FALSE
) 
@

\chapter{Results}

\section{Important questions when forecasting recruitment at the design-stage of a study}

By normal approximation to the Poisson distribution $C(t)\sim \textrm{N}(\mu=\lambda t, \sigma^2=(\lambda t)^2)$, we know that the probability of recruiting the desired $N$ participants is 0.5. Which means that the study has 50\% chance of obtaining the desired sample size in the suggested $T$ \citep{carter2004application}. We would also be assuming that the recruitment rate is constant over time.

In fact, we do not need normal approximation to see this. This can be shown with the Poisson distribution itself. We only need to specify the probability above $\lambda t$, for example, 0.5. For large $\lambda$, 50\% of the distribution will be below $\lambda t$. With $\lambda < 1$ this is no longer the case. 

This raises two questions which will be answered throughout this Master Thesis:
\begin{enumerate}
\item \textbf{Rate:} If $T$ is fixed, what does the expected rate $\lambda$ need to be to achieve a certain certainty of enrolling the total sample size $N$ within the time frame $T$?
\item \textbf{Time:} Given a certain rate $\lambda$, how long should the recruitment period $T$ be planned to give a confidence above 50\% of recruiting the total sample size $N$? In Machine Learning, this confidence is aimed at 80\%. In \cite{carter2004application}, at 90\%.
\end{enumerate}

\section{Pros and cons of Monte Carlo's simulations}

Carter suggests using Monte Carlo simulations, independent and identically distributed realizations of random variables. One clear advantage is its flexibility, as we can simulate any distribution we want. However, we must consider the following when we compute MC sampling instead of exact:

\begin{itemize}
\item $M$, the number of simulations
\item Set a seed for computational reproducibility
\item Monte Carlo standard errors (MCse) of estimates based on MC simulations
\item Pseudo random numbers generated in R rely in the assumption that these pseudo random numbers are close to the true realizations of random variables \citep{held2014applied}
\end{itemize}
\section{Counts: Comparison exact vs Monte Carlo simulations}

Carter raises two important questions, enumerated in the previous section \citep{carter2004application, carter2005practical}. He suggests the use of Monte Carlo (MC) simulations for Counts. Here we investigate the accuracy of his MC simulations by comparing them with exact distributions for accrual of counts introduced in Chapter 2. 

In Figures \ref{fig:3_1} and \ref{fig:3_2}, we can see how for $n=10^5$, MC sampling converges to the theoretical approaches discussed in Chapter 2.
% 
% <<echo = TRUE, cache=TRUE>>=
% set.seed(2025)
% 
% M <- 10^5
% t <- seq(1, 550, 1)
% lambda <- 0.591
% 
% # Generate cumulative Poisson paths
% cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
% 
% for (i in 1:M) {
% 	cval <- rpois(length(t), lambda)
% 	cval_cum_matrix[i, ] <- cumsum(cval)
% }
% 
% # Extract final counts for histogram
% final_counts <- cval_cum_matrix[, length(t)]
% @

% 
% \begin{figure}
% <<echo=FALSE, cache = TRUE, warning=FALSE>>=
% set.seed(2025)
% 
% M <- 10^5
% t <- seq(1, 550, 1)
% lambda <- 0.591
% 
% # Generate cumulative Poisson paths
% cval_cum_matrix <- matrix(NA, nrow = M, ncol = length(t))
% 
% for (i in 1:M) {
% 	cval <- rpois(length(t), lambda)  
% 	cval_cum_matrix[i, ] <- cumsum(cval)  
% }
% 
% # Extract final counts for histogram
% final_counts <- cval_cum_matrix[, length(t)]
% 
% # Plot the density estimate
% plot(density(final_counts),
% 		 main = "MC sampling vs Theoretical PMF",
% 		 xlab = "Counts",
% 		 col = "purple", 
% 		 lwd = 4)
% 
% # Overlay the Poisson PMF as points or lines
% x_vals <- 200:400
% lines(x_vals, dpois(x_vals, lambda = lambda * 550), 
% 			lwd = 2,
% 			lty = 2,
% 			col = "blue")
% legend("topright", 
% 			 legend = c("MC sampling", "Theoretical Poisson"), 
% 			 col = c("purple", "blue"),
% 			 lwd = c(4, 2),
% 			 lty = c(1, 2), 
% 			 cex = 0.7)
% @
%   \caption{Comparison of theoretical Probability Mass Function (PMF) of Poisson model for counts centered at $\lambda = 0.591$ for accrual at time $t=550$ and Monte Carlo (MC) sampling with $M=10^5$.}
%   \label{fig:3_1}
% \end{figure}
% 
% 
% \begin{figure}
% <<echo=FALSE, cache = TRUE, warning=FALSE>>=
% set.seed(2025)
% 
% n <- 10^5
% t <- seq(1, 550, 1)
% lambda <- 0.591
% 
% alpha <- 324
% beta <- 548
% # Generate cumulative Poisson paths
% cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
% v_lambda <- rgamma(n, shape = alpha, rate = beta)
% 
% for (i in 1:n) {
% 	cval <- rpois(length(t), lambda = v_lambda[i])
% 	cval_cum_matrix[i, ] <- cumsum(cval)  
% }
% 
% # Extract final counts for histogram
% final_counts <- cval_cum_matrix[, length(t)]
% 
% 
% # Plot the density estimate
% plot(density(final_counts),
% 		 main = "MC sampling vs Theoretical PMF",
% 		 xlab = "Counts",
% 		 col = "purple", 
% 		 lwd = 4)
% 
% # Overlay the Poisson PMF as points or lines
% x_vals <- 200:400
% lines(x_vals, dnbinom(x_vals, size = 324, mu = lambda * 550), 
% 			lwd = 2,
% 			lty = 2,
% 			col = "blue")
% legend("topright", 
% 			 legend = c("MC sampling", "Theoretical Poisson"), 
% 			 col = c("purple", "blue"),
% 			 lwd = c(4, 2),
% 			 lty = c(1, 2), 
% 			 cex = 0.7)
% 
% 
% @
% \caption{Comparison of theoretical Probability Mass Function (PMF) of Poisson-Gamma model for counts with $\alpha = 324$ and $\beta = 548$ for accrual at time $t=550$, and Monte Carlo (MC) sampling with $M=10^5$.}
% \label{fig:3_2}
% \end{figure}



\section{Time: Comparison exact vs Monte Carlo simulations}
%
% <<echo=TRUE, cache=TRUE, warning=FALSE>>=
% set.seed(2025)
% M <- 10^5
% Nneed <- 324
% lambda <- 0.591
%
% timep <- rep(0, M)
% csump <- rep(0, M)
%
% for(m in 1:M){
% 	while (csump[m] < Nneed) {
% 		csump[m] <- csump[m] + rpois(1, lambda)
% 		timep[m] <- timep[m] + 1
% 	}
% }
% @


% 
% \begin{figure}
% <<echo=FALSE>>=
% set.seed(2025)
% M <- 10^5
% Nneed <- 324
% lambda <- 0.591
% 
% alpha <- 324
% beta <- 548
% 
% timep <- rep(0, M)
% csump <- rep(0, M)
% 
% for(m in 1:M){
% 	while (csump[m] < Nneed) {
% 		csump[m] <- csump[m] + rpois(1, lambda)
% 		timep[m] <- timep[m] + 1
% 	}
% }
% 
% tval <- rgamma(n, shape = alpha, rate = lambda)
% 
% plot(density(tval),
% 		 main = "Density Time",
% 		 xlab = "Day",
% 		 col = "purple",
% 		 lwd = 4)
% lines(density(timep),
% 			lwd = 2,
% 			lty = 2,
% 			col = "blue")
% legend("topright",
% 			 legend = c("Theoretical Gamma", "MC sampling"),
% 			 col = c("purple", "blue"),
% 			 lwd = c(4, 2),
% 			 lty = c(1, 2),
% 			 cex = 0.7)
% 
% @
% \caption{Comparison of theoretical Density function of Gamma model for time with parameters $\alpha = 324$ and $\beta = 548$ and Monte Carlo (MC) sampling with $M=10^5$.}
% \label{fig:3_3}
% \end{figure}

% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% set.seed(2025)
% 
% M <- 10^5
% Nneed <- 324
% lambda <- 0.591
% alpha <- 324
% beta <- 548
% 
% timepg <- rep(0, M)
% csumpg <- rep(0, M)
% 
% for(m in 1:M){
% 	while (csumpg[m] < Nneed) {
% 		csumpg[m] <- csumpg[m] + rnbinom(1, size = alpha, prob = beta/(beta+1))
% 		timepg[m] <- timepg[m] + 1
% 	}
% }
% 
% v_lambda <- rgamma(n, shape = alpha, rate = beta)
% tval <- numeric(n)
% 
% for (i in 1:n) {
% 	tval[i] <- rgamma(1, shape = alpha, rate = v_lambda[i])
% }
% 
% plot(density(tval),
% 		 main = "Density Time",
% 		 xlab = "Day",
% 		 col = "purple",
% 		 lwd = 4)
% lines(density(timepg),
% 			lwd = 2,
% 			lty = 2,
% 			col = "blue")
% legend("topright",
% 			 legend = c("Theoretical Poisson", "MC sampling"),
% 			 col = c("purple", "blue"),
% 			 lwd = c(4, 2),
% 			 lty = c(1, 2),
% 			 cex = 0.7)
% @
% \caption{Comparison of theoretical Density function of Gamma-Gamma model for time with parameters $\alpha = 324$ and $\beta = 548$ and Monte Carlo (MC) sampling with $M=10^5$.}
% \label{fig:3_4}
% \end{figure}
% 
% 
% 
% 
% 
