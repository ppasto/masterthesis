% LaTeX file for Chapter 02
<<'preamble02',include=FALSE>>=
library(knitr) 
opts_chunk$set( 
    fig.path='figure/ch02_fig',    
    self.contained=FALSE,
    cache=!FALSE
) 
@


<<echo=FALSE>>=
library(knitr) 
opts_chunk$set( 
    fig.path='figure/ch02_fig',    
    self.contained=FALSE,
    cache=TRUE
) 
@



<<echo=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/ch02_fig',
               echo=TRUE, message=FALSE,
               fig.width=7, fig.height=7,  
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
) 
knitr::opts_chunk$set(highlight = FALSE)
options(width=74)
@ 

\chapter{Methodology} 

\section{Definitions}

The general notion of \textbf{Recruitment} in this Master Thesis refers to the number of patients (Counts) at the Eligibility, or Enrollment, or Randomization, or Statistical Analysis stage in Figure \ref{fig:2_1_a}. We define  \textbf{Accrual} as cumulative recruitment.

The \textbf{Target Population} is a specific group within the broader population, defined by attributes relevant to the research question. This group is focused on criteria that match the study's goals \citep{willie2024population}. Defining the target population allows researchers to refine their objectives and recruitment methods to align with the study's aims.


The \textbf{Eligibility} criteria are the specific requirements that individuals must meet to participate in a study. Eligible patients will be selected from the target population. Inclusion criteria specify the conditions that allow individuals to participate in the trial, particularly focusing on the medical condition of interest. Any other factors that limit eligibility are classified as exclusion criteria \citep{van2007eligibility}, conditions or circumstances that disqualify potential participants \citep{food2018evaluating}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{whelan_flow.png}
  \caption{Patient flow-chart. A total of 111 patients with localized high-risk disease were ineligible due to diagnosis issues (n = 14), treatment-related toxicity or contraindications (n = 78), early radiotherapy (n = 13), or psychological problems (n = 6), while 123 eligible patients were not enrolled for other reasons. \citep{whelan2018high}.}
  \label{fig:2_0}
\end{figure}

In clinical trials, \textbf{Enrollment} refers to the formal process of registering participants into a study after they have met all eligibility criteria and provided informed consent. This process includes verifying that each participant satisfies the inclusion and exclusion criteria outlined in the study protocol \citep{NIH2021}. It is important to distinguish between recruitment and enrollment. Recruitment involves identifying and inviting potential participants to join the study, whereas enrollment occurs after these individuals have been screened, consented, and officially registered into the trial \citep{frank2004current}. 

Once enrolled, participants are assigned to specific treatment groups or interventions as defined by the study design. The most common practice is \textbf{Randomization}. In clinical research, randomization is the process of assigning participants to different treatment groups using chance methods, such as random number generators or coin flips \citep{lim2019randomization}. Randomized controlled trials (RCTs) are considered the most effective method for preventing bias in the evaluation of new interventions, drugs, or devices. \citep{van2007eligibility}.


In clinical research, \textbf{Statistical Analysis} involves applying statistical methods to collect, summarize, interpret, and present data derived from clinical studies. This process is essential for evaluating the safety, efficacy, and overall outcomes of medical interventions, ensuring that conclusions drawn are both reliable and valid \citep{panos2023statistical}. Not all participants who are randomized may be included in the final statistical analysis due to protocol deviations of patients not adhering to the protocol \citep{rehman2020exclusion}, missing data \citep{shih2002problems} or loss-to-follow-up, some participants may become unreachable or withdraw consent during the study, resulting in missing outcome data \citep{nuesch2009effects}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_a.png}
  \caption{Patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_a}
\end{figure}

The number of patients decreases at each stage of a clinical study, from defining the target population to final statistical analysis, see Figure \ref{fig:2_1_b}. This process is known as patient leakage \citep{desai2014preventing}, alternative terms are attrition or retention.

We generalized the notion of patient leakage found in trial profiles such as the one found in Figure \ref{fig:2_0}. We conceptualized the different stages in a clinical trial and studied the different reasons why we find patient attrition when moving to the following stage in a study.

Eligibility criteria narrow down participants, and enrollment further reduces numbers as only those meeting strict criteria are registered. Randomization assigns individuals to treatment groups, but some may later be excluded due to protocol deviations, missing data, or loss to follow-up. 


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_2_1_b.png}
  \caption{Visual representation of patient recruitment and leakage at each stage of a clinical study \citep{piantadosi2022principles, whelan2018high, bogin2022lasagna}.}
  \label{fig:2_1_b}
\end{figure}

\section{Uncertainty and models for counts}

There are two types of uncertainty, aleatory and epistemic \citep{ohagan2006}. The \textbf{Aleatory Uncertainty} reflects randomness that is inherent, irreducible and unpredictable in nature. \textbf{Epistemic Uncertainty} arises primarily from limited or imperfect knowledge about the parameters of a statistical model and can reflect fluctuations of the parameter. Obtaining more or better information about the parameter typically reduces the epistemic uncertainty. 


Let us denote

\begin{itemize}
\item $T=time$
\item $C=counts$
\item $\lambda=\frac{C}{T}$
\end{itemize}

We define \textbf{Recruitment Rate} $\lambda=\frac{C}{T}$ at which patients are collected, measured as persons per unit of time, where \textbf{Rate} is understood as a ratio in which the numerator and denominator are incremental differences \citep{piantadosi2024clinical}:

\begin{align*}
\lambda = \frac{\Delta C}{\Delta T} = \frac{C_1 - C_0}{T_1 - T_0} = \frac{C_1 - 0}{T_1 - 0} = \frac{C_1}{T_1}
\end{align*}


\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C = \lambda  $ & $\lambda  $ & 0 & No & No \\
Poisson & $C \sim \textrm{Po} (\lambda )$ & $\lambda $ & $\lambda  $ & Yes & No \\
Negative Binomial & $C \sim \textrm{Po} (\Lambda )$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha(\beta+1)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty of recruitment in one unit of time recruitment covered by different models for counts.}
\label{tab:count_modeling}
\end{table}




\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
 \textbf{Methods} & \textbf{Counts} & \textbf{Expectation} & \textbf{Variance} & \textbf{Aleatory} & \textbf{Epistemic} \\
\hline
\hline
Expectation & $C(t) = \lambda  t$ & $\lambda  t$ & 0 & No & No \\
Poisson & $C(t) \sim \textrm{Po} (\lambda  t)$ & $\lambda  t$ & $\lambda  t$ & Yes & No \\
Negative Binomial & $C(t) \sim \textrm{Po} (\Lambda  t)$; $\Lambda \sim \textrm{G}(\alpha,\beta)$ & $t\frac{\alpha}{\beta}$ & $t\frac{\alpha(\beta+t)}{\beta^2}$ & Yes & Yes \\
\end{tabular}
}
\caption{Moments and aleatory and epistemic uncertainty in accrual until $t$ covered by different models for counts.}
\label{tab:count_modeling_2}
\end{table}

Methods in Tables \ref{tab:count_modeling} and \ref{tab:count_modeling_2} are applicable to each level of recruitment in Figures \ref{fig:2_1_a} and \ref{fig:2_1_b}.


\section{Counts: Model based on Expectations}
\label{sec:expect}

If we fix the duration of a study at time $T$ and we expect that we collect $C$ patients until $T$, we deterministically predict the recruitment rate per one unit of time (without taking into consideration any uncertainty) to be $\hat{\lambda}=\frac{C}{T}$, \cite{carter2004application}.

This is a simple deterministic method based on a linear extrapolation of constant recruitment also called "First Order Recruitment Model" (FORM) (ref.). It is sometimes referred to as the \textit{unconditional approach} and its main limitation is the lack of a mechanism to account for known sources of variation in the rate, \cite{carter2005practical}. 

The model based on expectations is overly simplistic and fails to account for changes in center recruitment or the regulatory environment, \cite{barnard2010systematic}. Therefore, stochastic models that incorporate randomness in the enrollment process are more suitable than the widely used deterministic approach, \cite{zhang2012modeling}.



\subsection{Expected recruitment in one unit of time}
$C = \textrm{E}C = \textrm{E}\lambda = \lambda$\\
$\textrm{Var}(C) = \textrm{Var}(\lambda) = 0$

As we can see in Table \ref{tab:count_modeling}

\subsection{Expected accrual at time point $t$}

Assuming that recruitments in each unit of time are independent of each other we have:

$C(t) = \textrm{E}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = \textrm{E}(\lambda t) = \lambda t$\\
$\textrm{Var}(C(t)) = \textrm{Var}(\underbrace{C+\ldots+C}_{t \ \text{times}}) = t \textrm{Var}(\lambda) = 0$

Both the expected accrual and its zero-variance are recorded in Table \ref{tab:count_modeling_2}
and visualized in Figure \ref{fig:2_2} and Figure \ref{fig:2_5}.



\section{Counts: Model based on Poisson Process}

The Poisson distribution $C\sim \rm{Po} (\lambda)$ allows us to explain the recruitment of patients. It is a discrete variable that expresses the probability of a given number of events (in our case, patient recruitment) occurring in a fixed unit interval of time. We assume that these events occur with a known constant rate $\lambda$ and are independent of each other.

\begin{align*}
\textrm{P}[C=&c] = \frac{\lambda^c}{c!}e^{-\lambda} \\
&c = 0,1,2,\ldots
\end{align*}


One important property from the Poisson distribution is that it is infinitely divisible \citep{held2014applied}. If $X_i\sim \textrm{Po} (\lambda_i)$ for $i=1,\ldots, n$ are independent, then, $\sum_{i=1}^n X_i \sim \textrm{Po} \Big( \sum_{i=1}^n \lambda_i \Big)$.

\subsection{Recruitment in one unit of time}

The recruitment of patients in one unit of time follows $C\sim \textrm{Po} (\lambda)$ and the expectation and variance are:

\begin{align*}
\textrm{E}C & = \lambda \\
\textrm{Var}(C) & = \lambda
\end{align*}

As we can see in Table \ref{tab:count_modeling}

\subsection{Accrual at time point $t$}
At time point $t$, the accrual follows $C\sim \textrm{Po} (\lambda t)$. Using the infinitely divisible property from the Poisson applicable to independent random variables, $\underbrace{\textrm{Po} (\lambda) +\cdots +\textrm{Po} (\lambda)}_{t \ \text{times}} = \textrm{Po} (\lambda t)$. We assume that the recruitment of patients in t unit time intervals is independent from another. As we can see in Table \ref{tab:count_modeling_2}, the expectation and variance are the following:

\begin{align*}
\textrm{E}C(t) & = \lambda t \\
\textrm{Var}(C(t)) & = \lambda t
\end{align*}

For example, if we assume $\lambda = 0.591$ per day and $t=550$, we can show the accrual of 100 different studies in Figure \ref{fig:2_2} and the histogram at $t=550$ days. The exact distribution at $t=550$ is provided in Figure \ref{fig:2_3} and the Cummulative Distribution Function (CDF) in Figure \ref{fig:2_4}. The uncertainty bands based on the exact quantiles are displayed in Figure \ref{fig:2_5}.

\begin{figure}
<<echo=FALSE>>=
# 1000 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))

for (i in 1:n) {
	cval <- rpois(length(t), lambda)  
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins

plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count")

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qpois(p = 0.975, lambda*t), lty = 2, lwd = 2, col = "red")
lines(t, qpois(p = 0.025, lambda*t), lty = 2, lwd = 2, col = "red")

legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("red", "black"),
			 lty = c(2, 1),
			 lwd = c(2,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(28, 0.01, 2.1, 2.1))  # Minimize margins
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Poisson-distributed counts with $\lambda = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from section \ref{sec:expect}, while the red dashed lines indicate Poisson's 95\% aleatory uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_2}
\end{figure}


\begin{figure}
<<echo=FALSE>>=
bar_positions <- barplot(dpois(200:500, lambda*550), 
												 main = "Probability Mass Function (PMF)", 
												 xlab = "Counts", 
												 xaxt = "n")  

axis(1, at = seq(1, length(200:500), by = 50), labels = seq(200, 500, by = 50))  
@   
  \caption{Probability Mass Function (PMF) of Poisson-distributed counts: This bar plot represents the probability mass function (PMF) of counts ranging from 200 to 500, using a Poisson distribution $\textrm{Po}(\lambda t)$ with a rate parameter $\lambda = 0.591$ per day at time $t = 550$ days.}
  \label{fig:2_3}
\end{figure}

\begin{figure}
<<echo=FALSE>>=
probdist <- dpois(200:500, lambda*550)
cdf <- cumsum(probdist)
cdf_positions <- barplot(cdf, 
												 main = "Cummulative distribution", 
												 xlab = "Counts", 
												 xaxt = "n")
axis(1, at = seq(1, length(200:500), by = 50), labels = seq(200, 500, by = 50))  
@   
  \caption{Cumulative Distribution Function (CDF) of Poisson-distributed counts: The bar plot illustrates the cumulative probability distribution for counts within the range of 200 to 500, using a Poisson $\textrm{Po}(\lambda t)$ distribution with a rate parameter $\lambda = 0.591$ per day at time $t=550$ days.}
  \label{fig:2_4}
\end{figure}


\begin{figure}
<<echo=FALSE>>=

# Compute Poisson quantiles over time
q_975_line <- qpois(0.975, lambda * t)
q_900_line <- qpois(0.900, lambda * t)
q_750_line <- qpois(0.75, lambda * t)
q_250_line <- qpois(0.25, lambda * t)
q_100_line <- qpois(0.1, lambda * t)
q_025_line <- qpois(0.025, lambda * t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
		 main = "Quantiles")

# Fill the area with a green gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(0/255, 100/255, 0/255, 0.4), border = NA)    # Dark Green
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  

# Legend
legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]", 
			 					 "10th - 90th Percentile     [80%]", 
			 					 "25th - 75th Percentile     [50%]", 
			 					 "Expected Accrual"),
			 col = c(rgb(144/255, 238/255, 144/255, 0.4), rgb(50/255, 205/255, 50/255, 0.4), rgb(0/255, 100/255, 0/255, 0.4), "black"),
			 pch = c(15, 15, 15, NA), lty = c(NA, NA, NA, 1), lwd = c(NA, NA, NA, 2),
			 bg = "white",
			 cex = 0.7)
@   
  \caption{Predicted uncertainty bands for Poisson process with $\lambda = 0.591$ per day. The black line represents the expected accrual, while the green shaded regions indicate aleatory uncertainty: the dark green band spans the interquantile range (25th - 75th percentiles), the lighter green band cover the 10th - 90th percentile range and the light green the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_5}
\end{figure}

\section{Counts: Negative Binomial model derived from Poisson-Gamma model}

% The Negative Binomial $X\sim\textrm{NBin}(r,\pi)$ models the number of 
% \textit{trials} in a sequence of independent and identically distributed Bernouillis before $r$ \textit{successes} occur. Instead of representing the number of successes in $n$ trials like a $Y\sim \textrm{Bin} (n, \pi)$, with the Negative Binomial we are looking at how many trials will it take to obtain $r$ successes.
\begin{figure}
<<echo = FALSE,  fig.width=7, fig.height=5>>=
par(mfrow=c(1,3))
curve(dgamma(x, shape = 324, rate = 548), 
			from = 0, to = 1,
			main = "Prior", 
			xlab = "Recruitment rate", 
			ylab = "",
			col = "red")

curve(dgamma(x, shape = 32.4, rate = 54.8), add = TRUE,
			col = "blue")

curve(dgamma(x, shape = 3.24, rate = 5.48), add = TRUE,
			col = "green")

legend("topleft",
			 legend = c(expression("G (" ~ alpha == 324 ~ beta == 548 ~ ")"),
			 						expression("G (" ~ alpha == 32.4 ~ beta == 54.8 ~ ")"),
			 						expression("G (" ~ alpha == 3.24 ~ beta == 5.48 ~ ")")),
			 col = c("red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)


plot(0:5, dpois(0:5, lambda), 
		 type = "l",
		 main = "PMF", 
		 xlab = "Counts", 
		 ylab = "")


lines(0:5, dnbinom(0:5, size = 324, mu = lambda), col = "red")
lines(0:5, dnbinom(0:5, size = 32.4, mu = lambda), col = "blue")
lines(0:5, dnbinom(0:5, size = 3.24, mu = lambda), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(NBin ~ (alpha == 324 ~ beta == 548)),
                  expression(NBin ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(NBin ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)


plot(0:5, ppois(0:5, lambda), 
		 type = "l",
		 main = "CDF", 
		 xlab = "Counts", 
		 ylab = "")



lines(0:5, pnbinom(0:5, size = 324, mu = lambda), col = "red")
lines(0:5, pnbinom(0:5, size = 32.4, mu = lambda), col = "blue")
lines(0:5, pnbinom(0:5, size = 3.24, mu = lambda), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(NBin ~ (alpha == 324 ~ beta == 548)),
                  expression(NBin ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(NBin ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)

@
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Negative Binomial changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=1$.}
  \label{fig:2_6}
\end{figure}

\subsection{Recruitment in one unit of time}
Let $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$
\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{\lambda^c\exp(-\lambda)}{c!}\Bigg[\lambda^{\alpha-1}\exp(-\beta\lambda)\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c)}{c!\Gamma(\alpha) (\beta+1)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+1)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+1)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots
\end{align*}
Thus, $C\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+1}\Bigg)$



Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}C &= \textrm{E}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda] = \alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C} (C|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_C(C|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda] + \textrm{E}_{\Lambda}[\Lambda] \\
&=\alpha/\beta^2 + \alpha/\beta = \frac{\alpha(\beta+1)}{\beta^2}
\end{align*}


There are two different interpretations of the Negative Binomial, Failure-Based and Count-based.
\subsection{Failure-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}(r,\pi)$ models the number of \textbf{failures} before achieving a fixed number of \textbf{successes} in a sequence of Bernoulli trials. 
\item Parametrization:
	\begin{itemize}
	\item $n$: Number of successes to be achieved (fixed).
	\item $p$: Probability of success in each trial
	\item The random variable $X$ represents the number of failures before achieving $n$ successes.
	\end{itemize}
\item Probability Mass Function (PMF) \citep{R-stats}:
\begin{align*}
\textrm{P}(X=&x) = \frac{\Gamma(x+n)}{\Gamma(x) x!}  p^n(1-p)^x, \\
&x = 0,1,2, \ldots, n >0\\
&0<p\leq 1
\end{align*}
where $x$ is the number of failures.
\item Interpretation: In a sequence of independent binary trials with constant probability $p$ of observing a \textit{non-recruited} patient, $X$ is the number of \textit{recruited} patients observed at the time that $x$ \textit{non-recruited} patients are observed \citep{meeker2017statistical}.


\end{enumerate}

With respect to the parameters, $n>0$ represents the number of successes until 
the experiment is stopped. The success probability in each experiment is 
represented by $p\in[0,1]$.  In R the functions [-]\texttt{nbinom(..., size = $n$, prob = $p$)} relate to the random variable $X-r$, the number of successes (as opposed to the number of trials) until $r$ successes have been achieved \citep{held2014applied}. 

\begin{align*}
EX & = \frac{r(1-\pi)}{\pi}\\
Var(X) & = \frac{r(1-\pi)}{\pi^2}
\end{align*}

Since we will be using the Count-Based interpretation of the Negative Binomial \citep{hilbe2011negative}, our parametrization relates to R with $n = \alpha$ and $p = \frac{\beta}{\beta+1}$.

\subsection{Count-Based}
\begin{enumerate}
\item The Negative Binomial $X\sim\textrm{NBin}\Bigg(\alpha,\frac{\beta}{\beta+1}\Bigg)$ can also be seen as a Poisson-Gamma mixture, where the observed count data follows a Poisson distribution with a mean that itself follows a Gamma distribution, $C|\Lambda \sim \textrm{Po}(\Lambda)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$. 
\item Parametrization:
	\begin{itemize}
	\item $\mu = \frac{\alpha}{\beta}$: Mean of the distribution (expected number of occurrences).
	\item $\alpha$: Dispersion parameter, controlling the variance.
	\end{itemize}
\item Alternative formulation of the PMF \citep{hilbe2011negative}:
\begin{align*}
\textrm{P}(X=&c) = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\beta+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha}, \\
&c\geq 0 
\end{align*}
where $c$ is the counts.
\item Interpretation: This model is used to represent "recruitment proneness". The parameter $\mu$ represents the expected number of recruitments in a study \citep{johnson2005univariate}.

\end{enumerate}

In the count-based method we take into account the overdispersion of the data. When the variability in the observed data is greater than what is expected.

\begin{align*}
\textrm{E}X&=\mu\\
\textrm{Var}X&=\mu + \frac{\mu}{\beta}
\end{align*}

How do we get to our formulation of the PMF:

\begin{align*}
\textrm{P}(X=c) & = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\mu}{\alpha+\mu}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\mu}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha/\beta}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{c} \Bigg(\frac{\alpha}{\alpha\beta/\beta+\alpha/\beta}\Bigg)^{\alpha} \\
& = \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{\alpha}{\alpha\beta+\alpha}\Bigg)^{c} \Bigg(\frac{\beta\alpha}{\alpha\beta+\alpha}\Bigg)^{\alpha} \\
&= \binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+1}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+1}\Bigg)^{\alpha}
\end{align*}



% 
% \begin{align*}
% Mean &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\frac{\beta}{\beta+1}}\\
% &= \frac{\alpha(\beta+1)}{\beta(\beta+1)}\\
% &= \frac{\alpha}{\beta}
% \end{align*}
% 
% \begin{align*}
% Variance &= \frac{\alpha\bigg(1-\frac{\beta}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha\bigg (\frac{1}{\beta+1}\bigg)}{\bigg(\frac{\beta}{\beta+1}\bigg)^2}\\
% &= \frac{\alpha(\beta+1)^2}{\beta^2(\beta+1)}\\
% &= \frac{\alpha(\beta+1)}{\beta^2}
% \end{align*}


\begin{figure}
<<echo=FALSE>>=
# 100 PATHS
# Set parameters
set.seed(2025)

n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591

alpha <- 324
beta <- 548
# Generate cumulative Poisson paths
cval_cum_matrix <- matrix(NA, nrow = n, ncol = length(t))
v_lambda <- rgamma(n, shape = alpha, rate = beta)

for (i in 1:n) {
	cval <- rpois(length(t), lambda = v_lambda[i])
	cval_cum_matrix[i, ] <- cumsum(cval)  
}

# Extract final counts for histogram
final_counts <- cval_cum_matrix[, length(t)]

# Reset graphics device to avoid layout issues
# if (dev.cur() > 1) dev.off()

# Set up layout: Histogram (small, upper right) + Main plot
layout(matrix(c(1, 2), ncol = 2), widths = c(3, 1), heights = c(4, 1))  

# Plot the main accrual time series
par(mar = c(4.1, 4.1, 2.1, 2.1))  # Restore normal margins
plot(t,  cval_cum_matrix[1,], 
		 type="n", 
		 main = "Accrual of 100 studies", 
		 xlab = "Time", 
		 ylab = "Count")

for(i in 1:n){
	lines(t,  cval_cum_matrix[i,], col = "lightgray")
}

# Add reference lines
lines(t, lambda*t)
lines(t, qnbinom(p = 0.975, size = alpha, mu = lambda*t), lty = 2, col = "red")
lines(t, qnbinom(p = 0.025, size = alpha, mu = lambda*t), lty = 2, col = "red")

legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]",
			 					 "Expected Accrual"),
			 col = c("red", "black"),
			 lty = c(2, 1),
			 lwd = c(1,2),
			 bg = "white",
			 cex = 0.7)  

# Plot histogram in the smaller upper right section
par(mar = c(27, 0.1, 2.1, 1))  # Adjust margins for visibility
hist_bins <- seq(min(final_counts), max(final_counts), length.out = 15)
hist_data <- hist(final_counts, breaks = hist_bins, plot = FALSE)

barplot(hist_data$counts, horiz = TRUE, space = 0, col = "gray", 
				axes = FALSE, xlab = "", ylab = "")
@   
  \caption{Poisson-Gamma distributed counts with $\mu = 0.591$ per day and uncertainty range. The black line represents the point estimate of the expected accrual from section \ref{sec:expect}, while the red dashed lines indicate Negative Binomial's 95\% aleatory uncertainty. The histogram illustrates the distribution of observed counts in 100 studies at time $t = 550$ days \citep{spiegelhalter2011visualizing, pkgacc}.}
  \label{fig:2_7}
\end{figure}



\begin{figure}
<<echo=FALSE>>=
n <- 100
t <- seq(1, 550, 1)
lambda <- 0.591
alpha <- 324
beta <- 548

# Compute Poisson quantiles over time
q_975_line <- qnbinom(p = 0.975, size = alpha, mu = lambda*t)
q_900_line <- qnbinom(p = 0.900, size = alpha, mu = lambda*t)
q_750_line <- qnbinom(p = 0.75, size = alpha, mu = lambda*t)
q_250_line <- qnbinom(p = 0.25, size = alpha, mu = lambda*t)
q_100_line <- qnbinom(p = 0.1, size = alpha, mu = lambda*t)
q_025_line <- qnbinom(p = 0.025, size = alpha, mu = lambda*t)

# Define x and y axis limits
x_limits <- range(t)
y_limits <- range(cval_cum_matrix)  # Ensures all quantiles fit

# Create the plot
par(mar = c(4.1, 4.1, 2.1, 2.1))
plot(NA, NA, xlim = x_limits, ylim = y_limits, xlab = "Time", ylab = "Count",
		 main = "Quantiles")

# Fill the area with a green gradient
polygon(c(t, rev(t)), c(q_975_line, rev(q_900_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green
polygon(c(t, rev(t)), c(q_900_line, rev(q_750_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_750_line, rev(q_250_line)), col = rgb(0/255, 100/255, 0/255, 0.4), border = NA)    # Dark Green
polygon(c(t, rev(t)), c(q_250_line, rev(q_100_line)), col = rgb(50/255, 205/255, 50/255, 0.4), border = NA)  # Lime Green
polygon(c(t, rev(t)), c(q_100_line, rev(q_025_line)), col = rgb(144/255, 238/255, 144/255, 0.4), border = NA) # Light Green

# Add the mean trend line
lines(t, lambda * t, col = "black", lwd = 2)  

# Legend
legend("topleft",
			 legend = c("2.5th - 97.5th Percentile [95%]", 
			 					 "10th - 90th Percentile     [80%]", 
			 					 "25th - 75th Percentile     [50%]", 
			 					 "Expected Accrual"),
			 col = c(rgb(144/255, 238/255, 144/255, 0.4), rgb(50/255, 205/255, 50/255, 0.4), rgb(0/255, 100/255, 0/255, 0.4), "black"),
			 pch = c(15, 15, 15, NA), lty = c(NA, NA, NA, 1), lwd = c(NA, NA, NA, 2),
			 bg = "white",
			 cex = 0.7)
@   
  \caption{Predicted uncertainty bands for Poisson-Gamma process with $\mu = 0.591$ per day. The black line represents the expected accrual, while the green shaded regions indicate aleatory uncertainty: the dark green band spans the interquantile range (25th - 75th percentiles), the lighter green band cover the 10th - 90th percentile range and the light green the 2.5th - 97.5th percentile range \citep{spiegelhalter2011visualizing}.}
  \label{fig:2_8}
\end{figure}

\subsection{Sensitivity Analysis}

In Figure \ref{fig:2_6}, the Poisson distribution captures aleatory uncertainty, while the Gamma prior represents epistemic uncertainty. The Negative Binomial distribution incorporates both types of uncertainty. The sensitivity analysis, also shown in Figure \ref{fig:2_6}, highlights that while the expectation remains unchanged, smaller parameter values lead to greater overall uncertainty due to the increased variance introduced by the Gamma distribution. The smaller the $\beta$, the greater the variance.


\begin{align*}
\textrm{Var}(\textrm{G}(\alpha, \beta)) = \frac{\alpha}{\beta^2} =\frac{\textrm{E}(\textrm{G}(\alpha, \beta))}{\beta}
\end{align*}

In the Poisson-Gamma model we can interpret $\alpha$ as the parameter that represents the sample size and $\beta$ represents time.

\begin{figure}
<<echo = FALSE,  fig.width=7, fig.height=5>>=
par(mfrow=c(1,3))
curve(dgamma(x, shape = 324, rate = 548), 
			from = 0, to = 1,
			main = "Prior", 
			xlab = "Recruitment rate", 
			ylab = "",
			col = "red")

curve(dgamma(x, shape = 32.4, rate = 54.8), add = TRUE,
			col = "blue")

curve(dgamma(x, shape = 3.24, rate = 5.48), add = TRUE,
			col = "green")

legend("topleft",
			 legend = c(expression("G (" ~ alpha == 324 ~ beta == 548 ~ ")"),
			 						expression("G (" ~ alpha == 32.4 ~ beta == 54.8 ~ ")"),
			 						expression("G (" ~ alpha == 3.24 ~ beta == 5.48 ~ ")")),
			 col = c("red", "blue", "green"),
			 lty = c(1, 1),
			 bg = "white",
			 cex = 0.6)


plot(200:500, dpois(200:500, lambda*550), 
		 type = "l",
		 main = "PMF", 
		 xlab = "Counts", 
		 ylab = "")


lines(200:500, dnbinom(200:500, size = 324, mu = lambda*550), col = "red")
lines(200:500, dnbinom(200:500, size = 32.4, mu = lambda*550), col = "blue")
lines(200:500, dnbinom(200:500, size = 3.24, mu = lambda*550), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(NBin ~ (alpha == 324 ~ beta == 548)),
                  expression(NBin ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(NBin ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)


plot(200:500, ppois(200:500, lambda*550), 
		 type = "l",
		 main = "CDF", 
		 xlab = "Counts", 
		 ylab = "")



lines(200:500, pnbinom(200:500, size = 324, mu = lambda*550), col = "red")
lines(200:500, pnbinom(200:500, size = 32.4, mu = lambda*550), col = "blue")
lines(200:500, pnbinom(200:500, size = 3.24, mu = lambda*550), col = "green")


legend("topleft",
       legend = c(expression(Po(lambda == 0.591)),
                  expression(NBin ~ (alpha == 324 ~ beta == 548)),
                  expression(NBin ~ (alpha == 32.4 ~ beta == 54.8)),
                  expression(NBin ~ (alpha == 3.24 ~ beta == 5.48))),
       col = c("black", "red", "blue", "green"),
       lty = c(1, 1),
       bg = "white",
       cex = 0.6)

@
	\caption{Sensitivity analysis between Poisson distribution with $\lambda = 0.591$ and Negative Binomial changing parameters of Gamma prior that maintain same expectation $\frac{\alpha}{\beta} = 0.591$ with $t=550$.}
  \label{fig:2_6}
\end{figure}

\subsection{Accrual at time point $t$}
Let $C|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$

\begin{align*}
p(c)&=\int^\infty_0 p(c|\lambda) p(\lambda) d\lambda\\
&=\int^\infty_0 \frac{(\lambda t)^c\exp(-\lambda t)}{c!}\Bigg[(\lambda t)^{\alpha-1}\exp(-\beta\lambda t)\frac{\beta^\alpha}{\Gamma(\alpha)}\Bigg]d\lambda\\
&=\frac{\beta^\alpha t^c}{c!\Gamma(\alpha)}\int^\infty_0 \lambda^{\alpha+c-1}\exp(-\lambda t)\exp(-\lambda\beta)d\lambda\\
&=\frac{\beta^\alpha\Gamma(\alpha+c) t^c}{c!\Gamma(\alpha) (\beta+t)^{\alpha+c}}\underbrace{\int^\infty_0 \frac{(\beta+t)^{\alpha+c}}{\Gamma(\alpha+c)} \lambda^{\alpha+c-1}\exp(-(\beta+t)\lambda)d\lambda}_{=1}\\
&=\beta^\alpha t^c\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{1}{\beta+t}\Bigg)^{\alpha+c}\\
&=\binom{\alpha+c-1}{\alpha-1}\Bigg (\frac{t}{\beta+t}\Bigg)^{c} \Bigg(\frac{\beta}{\beta+t}\Bigg)^{\alpha}\\
& c = 0,1,2,3,\ldots
\end{align*}
Thus, $C\sim \textrm{NBin} \Bigg(\alpha, \frac{\beta}{\beta+t}\Bigg)$

We will be focusing on the count-based interpretation of the Negative Binomial distribution. We can relate this interpretation to the more popular failure-based interpretation by seeing $\alpha$ as $n$, the number of successes. In our case, the number of patients we wish to recruit. And, $p = \frac{\beta}{\beta+1}$ which is the probability of success, as the probability of recruiting.


Using the expressions of iterated expectation and variance \citep{held2014applied} and the expectation and variance from the respective random variables $C(t)|\Lambda \sim \textrm{Po}(\Lambda t)$ and $\Lambda \sim \textrm{G}(\alpha,\beta)$, we have that:


\begin{align*}
\textrm{E}(C(t)) &= \textrm{E}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] = \textrm{E}_{\Lambda}[\Lambda t] = t\alpha/\beta
\end{align*}

\begin{align*}
\textrm{Var}(C(t)) &= \textrm{Var}_{\Lambda}[\textrm{E}_{C(t)} (C(t)|\Lambda)] + \textrm{E}_{\Lambda}[\textrm{Var}_{C(t)}(C(t)|\Lambda)]\\
&=\textrm{Var}_{\Lambda}[\Lambda t] + \textrm{E}_{\Lambda}[\Lambda t] \\
&=t^2\alpha/\beta^2 + t\alpha/\beta = \frac{t\alpha(\beta+t)}{\beta^2}
\end{align*}


\section{Important questions when forecasting recruitment at the design-stage of a study}

By normal approximation to the Poisson distribution $C(t)\sim \textrm{N}(\mu=\lambda t, \sigma^2=(\lambda t)^2)$, we know that the probability of recruiting the desired $N$ participants is 0.5. Which means that the study has 50\% chance of obtaining the desired sample size in the suggested $T$ \citep{carter2004application}. We would also be assuming that the recruitment rate is constant over time.

In fact, we do not need normal approximation to see this. This can be shown with the Poisson distribution itself. We only need to specify the probability above $\lambda t$, for example, 0.5. For large $\lambda$, 50\% of the distribution will be below $\lambda t$. With $\lambda < 1$ this is no longer the case. 

This raises two questions which will be answered throughout this Master Thesis:
\begin{enumerate}
\item \textbf{Rate:} If $T$ is fixed, what does the expected rate $\lambda$ need to be to achieve a certain certainty of enrolling the total sample size $N$ within the time frame $T$?
\item \textbf{Time:} Given a certain rate $\lambda$, how long should the recruitment period $T$ be planned to give a confidence above 50\% of recruiting the total sample size $N$? In Machine Learning, this confidence is aimed at 80\%. In \cite{carter2004application}, at 90\%.
\end{enumerate}
